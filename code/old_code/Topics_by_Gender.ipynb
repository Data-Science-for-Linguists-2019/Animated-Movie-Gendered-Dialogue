{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis\n",
    "Do characters of different genders talk about different things? In this code I will...\n",
    "\n",
    "1) Lemmatize each line of text\n",
    "\n",
    "2) Use these lemmas to create topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary stuff\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_pickle(r\"C:/Users/cassi/Desktop/Data_Science/Animated-Movie-Gendered-Dialogue/private/all_tagged_dialogue.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13442 entries, 0 to 14095\n",
      "Data columns (total 17 columns):\n",
      "Disney_Period       13442 non-null object\n",
      "Gender              13442 non-null object\n",
      "Movie               13442 non-null object\n",
      "Role                13442 non-null object\n",
      "Song                13442 non-null object\n",
      "Speaker             13442 non-null object\n",
      "Speaker_Status      13442 non-null object\n",
      "Text                13442 non-null object\n",
      "UTTERANCE_NUMBER    13442 non-null int64\n",
      "Year                13442 non-null int64\n",
      "Tokens              13442 non-null object\n",
      "Types               13442 non-null object\n",
      "Token_Count         13442 non-null int64\n",
      "Type_Count          13442 non-null int64\n",
      "POS                 13442 non-null object\n",
      "Tag_Freq            13442 non-null object\n",
      "Command_Count       13442 non-null int64\n",
      "dtypes: int64(5), object(12)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "NLTK offers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slave in the magic mirror come from the farthest space through wind and darkness i summon thee. speak ! let me see thy face. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = movies_df.Text.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmmm. Well, I've tried installing pattern but without much success. Let's try another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slave',\n",
       " 'magic',\n",
       " 'mirror',\n",
       " 'come',\n",
       " 'farthest',\n",
       " 'space',\n",
       " 'wind',\n",
       " 'darkness',\n",
       " 'summon',\n",
       " 'let',\n",
       " 'see',\n",
       " 'thy',\n",
       " 'face']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = []\n",
    "for i in range(0,11):\n",
    "    sent_list.append(movies_df.Text.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas=[]\n",
    "for sent in sent_list:\n",
    "    lemmas.append([wd.decode('utf-8').split('/')[0] for wd in lemmatize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slave', 'magic', 'mirror', 'come', 'farthest', 'space', 'wind', 'darkness', 'summon', 'let', 'see', 'thy', 'face'] \n",
      "\n",
      "['wouldst', 'know', 'queen'] \n",
      "\n",
      "['magic', 'mirror', 'wall', 'be', 'fairest'] \n",
      "\n",
      "['fame', 'be', 'thy', 'beauty', 'majesty', 'hold', 'lovely', 'maid', 'see', 'rag', 'hide', 'gentle', 'grace', 'be', 'more', 'fair'] \n",
      "\n",
      "['reveal', 'name'] \n",
      "\n",
      "['lip', 'red', 'rise', 'hair', 'black', 'ebony', 'skin', 'white', 'snow'] \n",
      "\n",
      "['snow', 'white'] \n",
      "\n",
      "['today'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in lemmas:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper as ttw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     slave in the magic mirror come from the farthe...\n",
       "1                   what wouldst thou know, my queen ? \n",
       "2     magic mirror on the wall, who is the fairest o...\n",
       "3     famed is thy beauty, majesty. but hold, a love...\n",
       "4                      alas for her ! reveal her name. \n",
       "5     lips red as the rose. hair black as ebony. ski...\n",
       "6                                         snow white ! \n",
       "8                                                today \n",
       "9                                                 oh ! \n",
       "10                                              hello. \n",
       "11                                                 oh. \n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Text.iloc[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#remove punctuation maybe?\n",
    "symbols = \".!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TreeTaggerError",
     "evalue": "Can't locate TreeTagger directory (and no TAGDIR specified).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTreeTaggerError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6d820233ccf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtagger\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mttw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTAGLANG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\treetaggerwrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using treetaggerwrapper.py from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Note: TreeTagger process is started later, when really needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\treetaggerwrapper.py\u001b[0m in \u001b[0;36m_set_tagger\u001b[1;34m(self, kargs)\u001b[0m\n\u001b[0;32m   1046\u001b[0m                 logger.error(\"Can't locate TreeTagger directory (and \"\n\u001b[0;32m   1047\u001b[0m                              \"no TAGDIR specified).\")\n\u001b[1;32m-> 1048\u001b[1;33m                 raise TreeTaggerError(\"Can't locate TreeTagger directory (and \"\n\u001b[0m\u001b[0;32m   1049\u001b[0m                                       \"no TAGDIR specified).\")\n\u001b[0;32m   1050\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTreeTaggerError\u001b[0m: Can't locate TreeTagger directory (and no TAGDIR specified)."
     ]
    }
   ],
   "source": [
    "tagger  = ttw.TreeTagger(TAGLANG = 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can experiment with more lemmatizers later. For now, let's keep going with gensim's LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Lemmas'] = movies_df.Text.map(lambda x: [wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [slave, magic, mirror, come, farthest, space, ...\n",
       "1                                   [wouldst, know, queen]\n",
       "2                       [magic, mirror, wall, be, fairest]\n",
       "3        [fame, be, thy, beauty, majesty, hold, lovely,...\n",
       "4                                           [reveal, name]\n",
       "5        [lip, red, rise, hair, black, ebony, skin, whi...\n",
       "6                                            [snow, white]\n",
       "8                                                  [today]\n",
       "9                                                       []\n",
       "10                                                      []\n",
       "11                                                      []\n",
       "13       [take, far, forest, find, seclude, glade, pick...\n",
       "14                                          [yes, majesty]\n",
       "15                              [faithful, huntsman, kill]\n",
       "16                             [majesty, little, princess]\n",
       "17                                   [know, penalty, fail]\n",
       "18                                          [yes, majesty]\n",
       "19       [make, doubly, sure, do, not, fail, bring, heart]\n",
       "21       [matter, mama, papa, believe, re, lose, please...\n",
       "22                   [do, forgive, beg, highness, forgive]\n",
       "23                                       [don, understand]\n",
       "24                       [mad, jealous, ll, stop, nothing]\n",
       "25                                                      []\n",
       "26                                                 [queen]\n",
       "27                                                 [queen]\n",
       "28       [now, quick, child, run, run, away, hide, wood...\n",
       "29       [please, don, run, away, win, hurt, awfully, s...\n",
       "34       [really, feel, quite, happy, now, sure, ll, ge...\n",
       "35       [sleep, ground, tree, way, do, sure, nest, pos...\n",
       "36                                                    [do]\n",
       "                               ...                        \n",
       "14066                                        [never, come]\n",
       "14067                                       [fight, blast]\n",
       "14068                                     [alpha, protect]\n",
       "14069                    [never, cease, amaze, bud, thank]\n",
       "14070                  [toothless, know, doesn, wash, out]\n",
       "14071                                           [stormfly]\n",
       "14072                               [give, cuddle, grumpy]\n",
       "14073                                   [little, princess]\n",
       "14074                                     [miss, so, much]\n",
       "14075                  [don, ever, leave, again, hookfang]\n",
       "14076                                               [barf]\n",
       "14077                                                [not]\n",
       "14078    [be, pretty, fine, dragon, wrangling, back, th...\n",
       "14079             [know, gonna, need, somebody, look, now]\n",
       "14080                                                   []\n",
       "14081                                          [be, honor]\n",
       "14082                         [father, be, bit, proud, be]\n",
       "14083                 [thank, really, glad, re, here, mom]\n",
       "14084                                     [here, ll, stay]\n",
       "14085                                [see, tell, be, here]\n",
       "14086                   [still, do, hilarious, come, here]\n",
       "14087                                                [ooh]\n",
       "14088                            [chief, have, come, home]\n",
       "14089    [be, berk, bit, trampled, busted, cover, ice, ...\n",
       "14090    [attack, be, relentless, crazy, stop, even, mo...\n",
       "14091    [be, small, number, stand, something, bigger, ...\n",
       "14092          [be, voice, peace, bit, bit, change, world]\n",
       "14093    [see, have, something, don, sure, have, army, ...\n",
       "14094                                               [have]\n",
       "14095                                             [dragon]\n",
       "Name: Lemmas, Length: 13442, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"Len_Lemmas\"] = movies_df.Lemmas.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disney_Period</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Role</th>\n",
       "      <th>Song</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speaker_Status</th>\n",
       "      <th>Text</th>\n",
       "      <th>UTTERANCE_NUMBER</th>\n",
       "      <th>Year</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Types</th>\n",
       "      <th>Token_Count</th>\n",
       "      <th>Type_Count</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag_Freq</th>\n",
       "      <th>Command_Count</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Len_Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh !</td>\n",
       "      <td>10</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, !]</td>\n",
       "      <td>{!, oh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (!, .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>prince</td>\n",
       "      <td>PRINCE</td>\n",
       "      <td>hello.</td>\n",
       "      <td>11</td>\n",
       "      <td>1937</td>\n",
       "      <td>[hello, .]</td>\n",
       "      <td>{., hello}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(hello, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh.</td>\n",
       "      <td>12</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, .]</td>\n",
       "      <td>{oh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (., .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>butbut who ?</td>\n",
       "      <td>26</td>\n",
       "      <td>1937</td>\n",
       "      <td>[butbut, who, ?]</td>\n",
       "      <td>{?, who, butbut}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(butbut, NN), (who, WP), (?, .)]</td>\n",
       "      <td>{'NN': 1, 'WP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>happy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>gosh.</td>\n",
       "      <td>65</td>\n",
       "      <td>1937</td>\n",
       "      <td>[gosh, .]</td>\n",
       "      <td>{gosh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(gosh, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>sneezy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, ahahah</td>\n",
       "      <td>92</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, ahahah]</td>\n",
       "      <td>{,, oh, ahahah}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(oh, UH), (,, ,), (ahahah, JJ)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'JJ': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>happy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>whawhat's that ? that's it.</td>\n",
       "      <td>96</td>\n",
       "      <td>1937</td>\n",
       "      <td>[whawhat, 's, that, ?, that, 's, it, .]</td>\n",
       "      <td>{?, whawhat, that, 's, ., it}</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>[(whawhat, WP), ('s, VBZ), (that, DT), (?, .),...</td>\n",
       "      <td>{'WP': 1, 'VBZ': 2, 'DT': 2, '.': 2, 'PRP': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>doc</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>it's up there.</td>\n",
       "      <td>98</td>\n",
       "      <td>1937</td>\n",
       "      <td>[it, 's, up, there, .]</td>\n",
       "      <td>{there, ., 's, up, it}</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[(it, PRP), ('s, VBZ), (up, RP), (there, RB), ...</td>\n",
       "      <td>{'PRP': 1, 'VBZ': 1, 'RP': 1, 'RB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>bashful</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, gosh !</td>\n",
       "      <td>136</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, gosh, !]</td>\n",
       "      <td>{gosh, ,, oh, !}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (gosh, JJ), (!, .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'JJ': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>and you ?</td>\n",
       "      <td>139</td>\n",
       "      <td>1937</td>\n",
       "      <td>[and, you, ?]</td>\n",
       "      <td>{?, and, you}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(and, CC), (you, PRP), (?, .)]</td>\n",
       "      <td>{'CC': 1, 'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>doc</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, yeah.</td>\n",
       "      <td>149</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, yeah, .]</td>\n",
       "      <td>{,, oh, ., yeah}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (yeah, UH), (., .)]</td>\n",
       "      <td>{'UH': 2, ',': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>sneezy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>hey ! hey ! hey ! hey !</td>\n",
       "      <td>240</td>\n",
       "      <td>1937</td>\n",
       "      <td>[hey, !, hey, !, hey, !, hey, !]</td>\n",
       "      <td>{hey, !}</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>[(hey, NN), (!, .), (hey, NN), (!, .), (hey, N...</td>\n",
       "      <td>{'NN': 4, '.': 4}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>sneezy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, gosh !</td>\n",
       "      <td>293</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, gosh, !]</td>\n",
       "      <td>{gosh, ,, oh, !}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (gosh, JJ), (!, .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'JJ': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>grumpy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>huh !</td>\n",
       "      <td>301</td>\n",
       "      <td>1937</td>\n",
       "      <td>[huh, !]</td>\n",
       "      <td>{!, huh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(huh, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>queen</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>ah ! ah !</td>\n",
       "      <td>318</td>\n",
       "      <td>1937</td>\n",
       "      <td>[ah, !, ah, !]</td>\n",
       "      <td>{!, ah}</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[(ah, NN), (!, .), (ah, NN), (!, .)]</td>\n",
       "      <td>{'NN': 2, '.': 2}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh.</td>\n",
       "      <td>353</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, .]</td>\n",
       "      <td>{oh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (., .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh.</td>\n",
       "      <td>355</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, .]</td>\n",
       "      <td>{oh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (., .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>birds chirping</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>why?</td>\n",
       "      <td>7</td>\n",
       "      <td>1950</td>\n",
       "      <td>[why, ?]</td>\n",
       "      <td>{?, why}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(why, WRB), (?, .)]</td>\n",
       "      <td>{'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>mice</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>he!</td>\n",
       "      <td>16</td>\n",
       "      <td>1950</td>\n",
       "      <td>[he, !]</td>\n",
       "      <td>{!, he}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(he, PRP), (!, .)]</td>\n",
       "      <td>{'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>mice</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>up!</td>\n",
       "      <td>54</td>\n",
       "      <td>1950</td>\n",
       "      <td>[up, !]</td>\n",
       "      <td>{!, up}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(up, RB), (!, .)]</td>\n",
       "      <td>{'RB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>gus</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>up!</td>\n",
       "      <td>55</td>\n",
       "      <td>1950</td>\n",
       "      <td>[up, !]</td>\n",
       "      <td>{!, up}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(up, RB), (!, .)]</td>\n",
       "      <td>{'RB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drizella</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>why, that's us!</td>\n",
       "      <td>148</td>\n",
       "      <td>1950</td>\n",
       "      <td>[why, ,, that, 's, us, !]</td>\n",
       "      <td>{,, that, !, us, 's, why}</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[(why, WRB), (,, ,), (that, DT), ('s, VBZ), (u...</td>\n",
       "      <td>{'WRB': 1, ',': 1, 'DT': 1, 'VBZ': 1, 'PRP': 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drizella</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh \"if\"</td>\n",
       "      <td>160</td>\n",
       "      <td>1950</td>\n",
       "      <td>[oh, ``, if, '']</td>\n",
       "      <td>{if, ``, oh, ''}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (``, ``), (if, IN), ('', '')]</td>\n",
       "      <td>{'UH': 1, '``': 1, 'IN': 1, '''': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>mouse</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh!</td>\n",
       "      <td>203</td>\n",
       "      <td>1950</td>\n",
       "      <td>[oh, !]</td>\n",
       "      <td>{!, oh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (!, .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh no!</td>\n",
       "      <td>225</td>\n",
       "      <td>1950</td>\n",
       "      <td>[oh, no, !]</td>\n",
       "      <td>{no, !, oh}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(oh, UH), (no, DT), (!, .)]</td>\n",
       "      <td>{'UH': 1, 'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>horse</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>ahem</td>\n",
       "      <td>265</td>\n",
       "      <td>1950</td>\n",
       "      <td>[ahem]</td>\n",
       "      <td>{ahem}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(ahem, NN)]</td>\n",
       "      <td>{'NN': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>another one?</td>\n",
       "      <td>271</td>\n",
       "      <td>1950</td>\n",
       "      <td>[another, one, ?]</td>\n",
       "      <td>{another, ?, one}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(another, DT), (one, CD), (?, .)]</td>\n",
       "      <td>{'DT': 1, 'CD': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>but uh</td>\n",
       "      <td>273</td>\n",
       "      <td>1950</td>\n",
       "      <td>[but, uh]</td>\n",
       "      <td>{but, uh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(but, CC), (uh, NN)]</td>\n",
       "      <td>{'CC': 1, 'NN': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>grand duke</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>ahem.</td>\n",
       "      <td>306</td>\n",
       "      <td>1950</td>\n",
       "      <td>[ahem, .]</td>\n",
       "      <td>{., ahem}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(ahem, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>prince</td>\n",
       "      <td>PRINCE</td>\n",
       "      <td>but why?</td>\n",
       "      <td>317</td>\n",
       "      <td>1950</td>\n",
       "      <td>[but, why, ?]</td>\n",
       "      <td>{?, but, why}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(but, CC), (why, WRB), (?, .)]</td>\n",
       "      <td>{'CC': 1, 'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>snotlout</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>yeah...</td>\n",
       "      <td>169</td>\n",
       "      <td>2014</td>\n",
       "      <td>[yeah, ...]</td>\n",
       "      <td>{..., yeah}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(yeah, NN), (..., :)]</td>\n",
       "      <td>{'NN': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13560</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what? why?!</td>\n",
       "      <td>173</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ?, why, ?, !]</td>\n",
       "      <td>{?, !, what, why}</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[(what, WP), (?, .), (why, WRB), (?, .), (!, .)]</td>\n",
       "      <td>{'WP': 1, '.': 3, 'WRB': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>why?</td>\n",
       "      <td>216</td>\n",
       "      <td>2014</td>\n",
       "      <td>[why, ?]</td>\n",
       "      <td>{?, why}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(why, WRB), (?, .)]</td>\n",
       "      <td>{'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13609</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>may i?</td>\n",
       "      <td>222</td>\n",
       "      <td>2014</td>\n",
       "      <td>[may, i, ?]</td>\n",
       "      <td>{i, ?, may}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(may, MD), (i, VB), (?, .)]</td>\n",
       "      <td>{'MD': 1, 'VB': 1, '.': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>ruffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, my.</td>\n",
       "      <td>228</td>\n",
       "      <td>2014</td>\n",
       "      <td>[oh, ,, my, .]</td>\n",
       "      <td>{., ,, oh, my}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (my, PRP$), (., .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'PRP$': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13628</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no.</td>\n",
       "      <td>241</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, .]</td>\n",
       "      <td>{no, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (., .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13728</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>wow.</td>\n",
       "      <td>341</td>\n",
       "      <td>2014</td>\n",
       "      <td>[wow, .]</td>\n",
       "      <td>{wow, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(wow, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13778</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>astrid</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what's down there?</td>\n",
       "      <td>391</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, 's, down, there, ?]</td>\n",
       "      <td>{?, what, down, there, 's}</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[(what, WP), ('s, VBZ), (down, RB), (there, RB...</td>\n",
       "      <td>{'WP': 1, 'VBZ': 1, 'RB': 2, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13781</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>drago!</td>\n",
       "      <td>394</td>\n",
       "      <td>2014</td>\n",
       "      <td>[drago, !]</td>\n",
       "      <td>{!, drago}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(drago, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13787</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>ruffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>hey!</td>\n",
       "      <td>400</td>\n",
       "      <td>2014</td>\n",
       "      <td>[hey, !]</td>\n",
       "      <td>{hey, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(hey, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13806</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>tuffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>nope.</td>\n",
       "      <td>419</td>\n",
       "      <td>2014</td>\n",
       "      <td>[nope, .]</td>\n",
       "      <td>{nope, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(nope, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13814</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>drago!</td>\n",
       "      <td>427</td>\n",
       "      <td>2014</td>\n",
       "      <td>[drago, !]</td>\n",
       "      <td>{!, drago}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(drago, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13820</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>astrid</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>433</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{!, no}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13825</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>we?</td>\n",
       "      <td>438</td>\n",
       "      <td>2014</td>\n",
       "      <td>[we, ?]</td>\n",
       "      <td>{?, we}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(we, PRP), (?, .)]</td>\n",
       "      <td>{'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13896</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, no...</td>\n",
       "      <td>509</td>\n",
       "      <td>2014</td>\n",
       "      <td>[oh, ,, no, ...]</td>\n",
       "      <td>{no, ,, oh, ...}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (no, DT), (..., :)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'DT': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13904</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drago</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what?!</td>\n",
       "      <td>517</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ?, !]</td>\n",
       "      <td>{?, !, what}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(what, WP), (?, .), (!, .)]</td>\n",
       "      <td>{'WP': 1, '.': 2}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13927</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>another one?</td>\n",
       "      <td>540</td>\n",
       "      <td>2014</td>\n",
       "      <td>[another, one, ?]</td>\n",
       "      <td>{another, ?, one}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(another, DT), (one, CD), (?, .)]</td>\n",
       "      <td>{'DT': 1, 'CD': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13938</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>valka</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>551</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{!, no}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13941</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>stoick</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>554</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{!, no}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13958</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>stoick</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what...?</td>\n",
       "      <td>571</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ..., ?]</td>\n",
       "      <td>{?, what, ...}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(what, WP), (..., :), (?, .)]</td>\n",
       "      <td>{'WP': 1, ':': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13975</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no... you...</td>\n",
       "      <td>588</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, ..., you, ...]</td>\n",
       "      <td>{no, you, ...}</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>[(no, DT), (..., :), (you, PRP), (..., :)]</td>\n",
       "      <td>{'DT': 1, ':': 2, 'PRP': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13991</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>tuffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>uh, with what?</td>\n",
       "      <td>604</td>\n",
       "      <td>2014</td>\n",
       "      <td>[uh, ,, with, what, ?]</td>\n",
       "      <td>{?, ,, what, with, uh}</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[(uh, NN), (,, ,), (with, IN), (what, WP), (?,...</td>\n",
       "      <td>{'NN': 1, ',': 1, 'IN': 1, 'WP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14012</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no...</td>\n",
       "      <td>625</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, ...]</td>\n",
       "      <td>{no, ...}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (..., :)]</td>\n",
       "      <td>{'DT': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>tuffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>uh... how?</td>\n",
       "      <td>628</td>\n",
       "      <td>2014</td>\n",
       "      <td>[uh, ..., how, ?]</td>\n",
       "      <td>{?, how, uh, ...}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(uh, NN), (..., :), (how, WRB), (?, .)]</td>\n",
       "      <td>{'NN': 1, ':': 1, 'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14020</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drago</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what?</td>\n",
       "      <td>633</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ?]</td>\n",
       "      <td>{?, what}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(what, WP), (?, .)]</td>\n",
       "      <td>{'WP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14024</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>snotlout</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>uh-oh...</td>\n",
       "      <td>637</td>\n",
       "      <td>2014</td>\n",
       "      <td>[uh-oh, ...]</td>\n",
       "      <td>{uh-oh, ...}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(uh-oh, JJ), (..., :)]</td>\n",
       "      <td>{'JJ': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14036</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drago</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>649</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{!, no}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14037</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>yeah!</td>\n",
       "      <td>650</td>\n",
       "      <td>2014</td>\n",
       "      <td>[yeah, !]</td>\n",
       "      <td>{!, yeah}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(yeah, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, no!</td>\n",
       "      <td>672</td>\n",
       "      <td>2014</td>\n",
       "      <td>[oh, ,, no, !]</td>\n",
       "      <td>{no, ,, oh, !}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (no, DT), (!, .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14080</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>me?</td>\n",
       "      <td>693</td>\n",
       "      <td>2014</td>\n",
       "      <td>[me, ?]</td>\n",
       "      <td>{?, me}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(me, PRP), (?, .)]</td>\n",
       "      <td>{'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Disney_Period Gender                       Movie    Role Song  \\\n",
       "9             EARLY      f                  Snow White     PRO    D   \n",
       "10            EARLY      m                  Snow White     PRO    D   \n",
       "11            EARLY      f                  Snow White     PRO    D   \n",
       "25            EARLY      f                  Snow White     PRO    D   \n",
       "64            EARLY      m                  Snow White  HELPER    D   \n",
       "91            EARLY      m                  Snow White  HELPER    D   \n",
       "95            EARLY      m                  Snow White  HELPER    D   \n",
       "97            EARLY      m                  Snow White  HELPER    D   \n",
       "135           EARLY      m                  Snow White  HELPER    D   \n",
       "138           EARLY      f                  Snow White     PRO    D   \n",
       "148           EARLY      m                  Snow White  HELPER    D   \n",
       "239           EARLY      m                  Snow White  HELPER    D   \n",
       "292           EARLY      m                  Snow White  HELPER    D   \n",
       "300           EARLY      m                  Snow White  HELPER    D   \n",
       "317           EARLY      f                  Snow White     ANT    D   \n",
       "352           EARLY      f                  Snow White     PRO    D   \n",
       "354           EARLY      f                  Snow White     PRO    D   \n",
       "369           EARLY      n                  Cinderella       N    D   \n",
       "378           EARLY      n                  Cinderella  HELPER    D   \n",
       "416           EARLY      n                  Cinderella  HELPER    D   \n",
       "417           EARLY      m                  Cinderella  HELPER    D   \n",
       "510           EARLY      f                  Cinderella     ANT    D   \n",
       "522           EARLY      f                  Cinderella     ANT    D   \n",
       "565           EARLY      n                  Cinderella  HELPER    D   \n",
       "587           EARLY      f                  Cinderella     PRO    D   \n",
       "627           EARLY      n                  Cinderella       N    D   \n",
       "633           EARLY      f                  Cinderella     PRO    D   \n",
       "635           EARLY      f                  Cinderella     PRO    D   \n",
       "668           EARLY      m                  Cinderella       N    D   \n",
       "679           EARLY      m                  Cinderella     PRO    D   \n",
       "...             ...    ...                         ...     ...  ...   \n",
       "13556    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13560    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13603    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13609    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13615    DREAMWORKS      f  How to Train Your Dragon 2  HELPER    D   \n",
       "13628    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13728    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13778    DREAMWORKS      f  How to Train Your Dragon 2     PRO    D   \n",
       "13781    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13787    DREAMWORKS      f  How to Train Your Dragon 2  HELPER    D   \n",
       "13806    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13814    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13820    DREAMWORKS      f  How to Train Your Dragon 2     PRO    D   \n",
       "13825    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13896    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13904    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13927    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13938    DREAMWORKS      f  How to Train Your Dragon 2  HELPER    D   \n",
       "13941    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13958    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13975    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13991    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "14012    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "14015    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "14020    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "14024    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "14036    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "14037    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "14059    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "14080    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "\n",
       "              Speaker Speaker_Status                          Text  \\\n",
       "9          snow white       PRINCESS                         oh !    \n",
       "10             prince         PRINCE                       hello.    \n",
       "11         snow white       PRINCESS                          oh.    \n",
       "25         snow white       PRINCESS                 butbut who ?    \n",
       "64              happy          NON-P                        gosh.    \n",
       "91             sneezy          NON-P                   oh, ahahah    \n",
       "95              happy          NON-P  whawhat's that ? that's it.    \n",
       "97                doc          NON-P               it's up there.    \n",
       "135           bashful          NON-P                   oh, gosh !    \n",
       "138        snow white       PRINCESS                    and you ?    \n",
       "148               doc          NON-P                    oh, yeah.    \n",
       "239            sneezy          NON-P      hey ! hey ! hey ! hey !    \n",
       "292            sneezy          NON-P                   oh, gosh !    \n",
       "300            grumpy          NON-P                        huh !    \n",
       "317             queen          NON-P                    ah ! ah !    \n",
       "352        snow white       PRINCESS                          oh.    \n",
       "354        snow white       PRINCESS                          oh.    \n",
       "369    birds chirping          NON-P                         why?    \n",
       "378              mice          NON-P                          he!    \n",
       "416              mice          NON-P                          up!    \n",
       "417               gus          NON-P                          up!    \n",
       "510          drizella          NON-P              why, that's us!    \n",
       "522          drizella          NON-P                      oh \"if\"    \n",
       "565             mouse          NON-P                          oh!    \n",
       "587        cinderella       PRINCESS                       oh no!    \n",
       "627             horse          NON-P                         ahem    \n",
       "633        cinderella       PRINCESS                 another one?    \n",
       "635        cinderella       PRINCESS                       but uh    \n",
       "668        grand duke          NON-P                        ahem.    \n",
       "679            prince         PRINCE                     but why?    \n",
       "...               ...            ...                           ...   \n",
       "13556        snotlout          NON-P                       yeah...   \n",
       "13560          hiccup          NON-P                   what? why?!   \n",
       "13603            eret          NON-P                          why?   \n",
       "13609          hiccup          NON-P                        may i?   \n",
       "13615         ruffnut          NON-P                       oh, my.   \n",
       "13628          hiccup          NON-P                           no.   \n",
       "13728          hiccup          NON-P                          wow.   \n",
       "13778          astrid          NON-P            what's down there?   \n",
       "13781            eret          NON-P                        drago!   \n",
       "13787         ruffnut          NON-P                          hey!   \n",
       "13806         tuffnut          NON-P                         nope.   \n",
       "13814            eret          NON-P                        drago!   \n",
       "13820          astrid          NON-P                           no!   \n",
       "13825          hiccup          NON-P                           we?   \n",
       "13896          hiccup          NON-P                     oh, no...   \n",
       "13904           drago          NON-P                        what?!   \n",
       "13927          hiccup          NON-P                  another one?   \n",
       "13938           valka          NON-P                           no!   \n",
       "13941          stoick          NON-P                           no!   \n",
       "13958          stoick          NON-P                      what...?   \n",
       "13975          hiccup          NON-P                  no... you...   \n",
       "13991         tuffnut          NON-P                uh, with what?   \n",
       "14012          hiccup          NON-P                         no...   \n",
       "14015         tuffnut          NON-P                    uh... how?   \n",
       "14020           drago          NON-P                         what?   \n",
       "14024        snotlout          NON-P                      uh-oh...   \n",
       "14036           drago          NON-P                           no!   \n",
       "14037          hiccup          NON-P                         yeah!   \n",
       "14059          hiccup          NON-P                       oh, no!   \n",
       "14080            eret          NON-P                           me?   \n",
       "\n",
       "       UTTERANCE_NUMBER  Year                                   Tokens  \\\n",
       "9                    10  1937                                  [oh, !]   \n",
       "10                   11  1937                               [hello, .]   \n",
       "11                   12  1937                                  [oh, .]   \n",
       "25                   26  1937                         [butbut, who, ?]   \n",
       "64                   65  1937                                [gosh, .]   \n",
       "91                   92  1937                          [oh, ,, ahahah]   \n",
       "95                   96  1937  [whawhat, 's, that, ?, that, 's, it, .]   \n",
       "97                   98  1937                   [it, 's, up, there, .]   \n",
       "135                 136  1937                         [oh, ,, gosh, !]   \n",
       "138                 139  1937                            [and, you, ?]   \n",
       "148                 149  1937                         [oh, ,, yeah, .]   \n",
       "239                 240  1937         [hey, !, hey, !, hey, !, hey, !]   \n",
       "292                 293  1937                         [oh, ,, gosh, !]   \n",
       "300                 301  1937                                 [huh, !]   \n",
       "317                 318  1937                           [ah, !, ah, !]   \n",
       "352                 353  1937                                  [oh, .]   \n",
       "354                 355  1937                                  [oh, .]   \n",
       "369                   7  1950                                 [why, ?]   \n",
       "378                  16  1950                                  [he, !]   \n",
       "416                  54  1950                                  [up, !]   \n",
       "417                  55  1950                                  [up, !]   \n",
       "510                 148  1950                [why, ,, that, 's, us, !]   \n",
       "522                 160  1950                         [oh, ``, if, '']   \n",
       "565                 203  1950                                  [oh, !]   \n",
       "587                 225  1950                              [oh, no, !]   \n",
       "627                 265  1950                                   [ahem]   \n",
       "633                 271  1950                        [another, one, ?]   \n",
       "635                 273  1950                                [but, uh]   \n",
       "668                 306  1950                                [ahem, .]   \n",
       "679                 317  1950                            [but, why, ?]   \n",
       "...                 ...   ...                                      ...   \n",
       "13556               169  2014                              [yeah, ...]   \n",
       "13560               173  2014                     [what, ?, why, ?, !]   \n",
       "13603               216  2014                                 [why, ?]   \n",
       "13609               222  2014                              [may, i, ?]   \n",
       "13615               228  2014                           [oh, ,, my, .]   \n",
       "13628               241  2014                                  [no, .]   \n",
       "13728               341  2014                                 [wow, .]   \n",
       "13778               391  2014               [what, 's, down, there, ?]   \n",
       "13781               394  2014                               [drago, !]   \n",
       "13787               400  2014                                 [hey, !]   \n",
       "13806               419  2014                                [nope, .]   \n",
       "13814               427  2014                               [drago, !]   \n",
       "13820               433  2014                                  [no, !]   \n",
       "13825               438  2014                                  [we, ?]   \n",
       "13896               509  2014                         [oh, ,, no, ...]   \n",
       "13904               517  2014                             [what, ?, !]   \n",
       "13927               540  2014                        [another, one, ?]   \n",
       "13938               551  2014                                  [no, !]   \n",
       "13941               554  2014                                  [no, !]   \n",
       "13958               571  2014                           [what, ..., ?]   \n",
       "13975               588  2014                      [no, ..., you, ...]   \n",
       "13991               604  2014                   [uh, ,, with, what, ?]   \n",
       "14012               625  2014                                [no, ...]   \n",
       "14015               628  2014                        [uh, ..., how, ?]   \n",
       "14020               633  2014                                [what, ?]   \n",
       "14024               637  2014                             [uh-oh, ...]   \n",
       "14036               649  2014                                  [no, !]   \n",
       "14037               650  2014                                [yeah, !]   \n",
       "14059               672  2014                           [oh, ,, no, !]   \n",
       "14080               693  2014                                  [me, ?]   \n",
       "\n",
       "                               Types  Token_Count  Type_Count  \\\n",
       "9                            {!, oh}            2           2   \n",
       "10                        {., hello}            2           2   \n",
       "11                           {oh, .}            2           2   \n",
       "25                  {?, who, butbut}            3           3   \n",
       "64                         {gosh, .}            2           2   \n",
       "91                   {,, oh, ahahah}            3           3   \n",
       "95     {?, whawhat, that, 's, ., it}            8           6   \n",
       "97            {there, ., 's, up, it}            5           5   \n",
       "135                 {gosh, ,, oh, !}            4           4   \n",
       "138                    {?, and, you}            3           3   \n",
       "148                 {,, oh, ., yeah}            4           4   \n",
       "239                         {hey, !}            8           2   \n",
       "292                 {gosh, ,, oh, !}            4           4   \n",
       "300                         {!, huh}            2           2   \n",
       "317                          {!, ah}            4           2   \n",
       "352                          {oh, .}            2           2   \n",
       "354                          {oh, .}            2           2   \n",
       "369                         {?, why}            2           2   \n",
       "378                          {!, he}            2           2   \n",
       "416                          {!, up}            2           2   \n",
       "417                          {!, up}            2           2   \n",
       "510        {,, that, !, us, 's, why}            6           6   \n",
       "522                 {if, ``, oh, ''}            4           4   \n",
       "565                          {!, oh}            2           2   \n",
       "587                      {no, !, oh}            3           3   \n",
       "627                           {ahem}            1           1   \n",
       "633                {another, ?, one}            3           3   \n",
       "635                        {but, uh}            2           2   \n",
       "668                        {., ahem}            2           2   \n",
       "679                    {?, but, why}            3           3   \n",
       "...                              ...          ...         ...   \n",
       "13556                    {..., yeah}            2           2   \n",
       "13560              {?, !, what, why}            5           4   \n",
       "13603                       {?, why}            2           2   \n",
       "13609                    {i, ?, may}            3           3   \n",
       "13615                 {., ,, oh, my}            4           4   \n",
       "13628                        {no, .}            2           2   \n",
       "13728                       {wow, .}            2           2   \n",
       "13778     {?, what, down, there, 's}            5           5   \n",
       "13781                     {!, drago}            2           2   \n",
       "13787                       {hey, !}            2           2   \n",
       "13806                      {nope, .}            2           2   \n",
       "13814                     {!, drago}            2           2   \n",
       "13820                        {!, no}            2           2   \n",
       "13825                        {?, we}            2           2   \n",
       "13896               {no, ,, oh, ...}            4           4   \n",
       "13904                   {?, !, what}            3           3   \n",
       "13927              {another, ?, one}            3           3   \n",
       "13938                        {!, no}            2           2   \n",
       "13941                        {!, no}            2           2   \n",
       "13958                 {?, what, ...}            3           3   \n",
       "13975                 {no, you, ...}            4           3   \n",
       "13991         {?, ,, what, with, uh}            5           5   \n",
       "14012                      {no, ...}            2           2   \n",
       "14015              {?, how, uh, ...}            4           4   \n",
       "14020                      {?, what}            2           2   \n",
       "14024                   {uh-oh, ...}            2           2   \n",
       "14036                        {!, no}            2           2   \n",
       "14037                      {!, yeah}            2           2   \n",
       "14059                 {no, ,, oh, !}            4           4   \n",
       "14080                        {?, me}            2           2   \n",
       "\n",
       "                                                     POS  \\\n",
       "9                                     [(oh, UH), (!, .)]   \n",
       "10                                 [(hello, NN), (., .)]   \n",
       "11                                    [(oh, UH), (., .)]   \n",
       "25                     [(butbut, NN), (who, WP), (?, .)]   \n",
       "64                                  [(gosh, NN), (., .)]   \n",
       "91                      [(oh, UH), (,, ,), (ahahah, JJ)]   \n",
       "95     [(whawhat, WP), ('s, VBZ), (that, DT), (?, .),...   \n",
       "97     [(it, PRP), ('s, VBZ), (up, RP), (there, RB), ...   \n",
       "135               [(oh, UH), (,, ,), (gosh, JJ), (!, .)]   \n",
       "138                      [(and, CC), (you, PRP), (?, .)]   \n",
       "148               [(oh, UH), (,, ,), (yeah, UH), (., .)]   \n",
       "239    [(hey, NN), (!, .), (hey, NN), (!, .), (hey, N...   \n",
       "292               [(oh, UH), (,, ,), (gosh, JJ), (!, .)]   \n",
       "300                                  [(huh, NN), (!, .)]   \n",
       "317                 [(ah, NN), (!, .), (ah, NN), (!, .)]   \n",
       "352                                   [(oh, UH), (., .)]   \n",
       "354                                   [(oh, UH), (., .)]   \n",
       "369                                 [(why, WRB), (?, .)]   \n",
       "378                                  [(he, PRP), (!, .)]   \n",
       "416                                   [(up, RB), (!, .)]   \n",
       "417                                   [(up, RB), (!, .)]   \n",
       "510    [(why, WRB), (,, ,), (that, DT), ('s, VBZ), (u...   \n",
       "522             [(oh, UH), (``, ``), (if, IN), ('', '')]   \n",
       "565                                   [(oh, UH), (!, .)]   \n",
       "587                         [(oh, UH), (no, DT), (!, .)]   \n",
       "627                                         [(ahem, NN)]   \n",
       "633                   [(another, DT), (one, CD), (?, .)]   \n",
       "635                                [(but, CC), (uh, NN)]   \n",
       "668                                 [(ahem, NN), (., .)]   \n",
       "679                      [(but, CC), (why, WRB), (?, .)]   \n",
       "...                                                  ...   \n",
       "13556                             [(yeah, NN), (..., :)]   \n",
       "13560   [(what, WP), (?, .), (why, WRB), (?, .), (!, .)]   \n",
       "13603                               [(why, WRB), (?, .)]   \n",
       "13609                       [(may, MD), (i, VB), (?, .)]   \n",
       "13615             [(oh, UH), (,, ,), (my, PRP$), (., .)]   \n",
       "13628                                 [(no, DT), (., .)]   \n",
       "13728                                [(wow, NN), (., .)]   \n",
       "13778  [(what, WP), ('s, VBZ), (down, RB), (there, RB...   \n",
       "13781                              [(drago, NN), (!, .)]   \n",
       "13787                                [(hey, NN), (!, .)]   \n",
       "13806                               [(nope, NN), (., .)]   \n",
       "13814                              [(drago, NN), (!, .)]   \n",
       "13820                                 [(no, DT), (!, .)]   \n",
       "13825                                [(we, PRP), (?, .)]   \n",
       "13896             [(oh, UH), (,, ,), (no, DT), (..., :)]   \n",
       "13904                       [(what, WP), (?, .), (!, .)]   \n",
       "13927                 [(another, DT), (one, CD), (?, .)]   \n",
       "13938                                 [(no, DT), (!, .)]   \n",
       "13941                                 [(no, DT), (!, .)]   \n",
       "13958                     [(what, WP), (..., :), (?, .)]   \n",
       "13975         [(no, DT), (..., :), (you, PRP), (..., :)]   \n",
       "13991  [(uh, NN), (,, ,), (with, IN), (what, WP), (?,...   \n",
       "14012                               [(no, DT), (..., :)]   \n",
       "14015           [(uh, NN), (..., :), (how, WRB), (?, .)]   \n",
       "14020                               [(what, WP), (?, .)]   \n",
       "14024                            [(uh-oh, JJ), (..., :)]   \n",
       "14036                                 [(no, DT), (!, .)]   \n",
       "14037                               [(yeah, NN), (!, .)]   \n",
       "14059               [(oh, UH), (,, ,), (no, DT), (!, .)]   \n",
       "14080                                [(me, PRP), (?, .)]   \n",
       "\n",
       "                                                Tag_Freq  Command_Count  \\\n",
       "9                                      {'UH': 1, '.': 1}              0   \n",
       "10                                     {'NN': 1, '.': 1}              0   \n",
       "11                                     {'UH': 1, '.': 1}              0   \n",
       "25                            {'NN': 1, 'WP': 1, '.': 1}              0   \n",
       "64                                     {'NN': 1, '.': 1}              0   \n",
       "91                            {'UH': 1, ',': 1, 'JJ': 1}              0   \n",
       "95        {'WP': 1, 'VBZ': 2, 'DT': 2, '.': 2, 'PRP': 1}              0   \n",
       "97        {'PRP': 1, 'VBZ': 1, 'RP': 1, 'RB': 1, '.': 1}              0   \n",
       "135                   {'UH': 1, ',': 1, 'JJ': 1, '.': 1}              0   \n",
       "138                          {'CC': 1, 'PRP': 1, '.': 1}              0   \n",
       "148                            {'UH': 2, ',': 1, '.': 1}              0   \n",
       "239                                    {'NN': 4, '.': 4}              0   \n",
       "292                   {'UH': 1, ',': 1, 'JJ': 1, '.': 1}              0   \n",
       "300                                    {'NN': 1, '.': 1}              0   \n",
       "317                                    {'NN': 2, '.': 2}              0   \n",
       "352                                    {'UH': 1, '.': 1}              0   \n",
       "354                                    {'UH': 1, '.': 1}              0   \n",
       "369                                   {'WRB': 1, '.': 1}              0   \n",
       "378                                   {'PRP': 1, '.': 1}              0   \n",
       "416                                    {'RB': 1, '.': 1}              0   \n",
       "417                                    {'RB': 1, '.': 1}              0   \n",
       "510    {'WRB': 1, ',': 1, 'DT': 1, 'VBZ': 1, 'PRP': 1...              0   \n",
       "522                 {'UH': 1, '``': 1, 'IN': 1, '''': 1}              0   \n",
       "565                                    {'UH': 1, '.': 1}              0   \n",
       "587                           {'UH': 1, 'DT': 1, '.': 1}              0   \n",
       "627                                            {'NN': 1}              0   \n",
       "633                           {'DT': 1, 'CD': 1, '.': 1}              0   \n",
       "635                                   {'CC': 1, 'NN': 1}              0   \n",
       "668                                    {'NN': 1, '.': 1}              0   \n",
       "679                          {'CC': 1, 'WRB': 1, '.': 1}              0   \n",
       "...                                                  ...            ...   \n",
       "13556                                  {'NN': 1, ':': 1}              0   \n",
       "13560                        {'WP': 1, '.': 3, 'WRB': 1}              0   \n",
       "13603                                 {'WRB': 1, '.': 1}              0   \n",
       "13609                         {'MD': 1, 'VB': 1, '.': 1}              1   \n",
       "13615               {'UH': 1, ',': 1, 'PRP$': 1, '.': 1}              0   \n",
       "13628                                  {'DT': 1, '.': 1}              0   \n",
       "13728                                  {'NN': 1, '.': 1}              0   \n",
       "13778               {'WP': 1, 'VBZ': 1, 'RB': 2, '.': 1}              0   \n",
       "13781                                  {'NN': 1, '.': 1}              0   \n",
       "13787                                  {'NN': 1, '.': 1}              0   \n",
       "13806                                  {'NN': 1, '.': 1}              0   \n",
       "13814                                  {'NN': 1, '.': 1}              0   \n",
       "13820                                  {'DT': 1, '.': 1}              0   \n",
       "13825                                 {'PRP': 1, '.': 1}              0   \n",
       "13896                 {'UH': 1, ',': 1, 'DT': 1, ':': 1}              0   \n",
       "13904                                  {'WP': 1, '.': 2}              0   \n",
       "13927                         {'DT': 1, 'CD': 1, '.': 1}              0   \n",
       "13938                                  {'DT': 1, '.': 1}              0   \n",
       "13941                                  {'DT': 1, '.': 1}              0   \n",
       "13958                          {'WP': 1, ':': 1, '.': 1}              0   \n",
       "13975                        {'DT': 1, ':': 2, 'PRP': 1}              0   \n",
       "13991        {'NN': 1, ',': 1, 'IN': 1, 'WP': 1, '.': 1}              0   \n",
       "14012                                  {'DT': 1, ':': 1}              0   \n",
       "14015                {'NN': 1, ':': 1, 'WRB': 1, '.': 1}              0   \n",
       "14020                                  {'WP': 1, '.': 1}              0   \n",
       "14024                                  {'JJ': 1, ':': 1}              0   \n",
       "14036                                  {'DT': 1, '.': 1}              0   \n",
       "14037                                  {'NN': 1, '.': 1}              0   \n",
       "14059                 {'UH': 1, ',': 1, 'DT': 1, '.': 1}              0   \n",
       "14080                                 {'PRP': 1, '.': 1}              0   \n",
       "\n",
       "      Lemmas  Len_Lemmas  \n",
       "9         []           0  \n",
       "10        []           0  \n",
       "11        []           0  \n",
       "25        []           0  \n",
       "64        []           0  \n",
       "91        []           0  \n",
       "95        []           0  \n",
       "97        []           0  \n",
       "135       []           0  \n",
       "138       []           0  \n",
       "148       []           0  \n",
       "239       []           0  \n",
       "292       []           0  \n",
       "300       []           0  \n",
       "317       []           0  \n",
       "352       []           0  \n",
       "354       []           0  \n",
       "369       []           0  \n",
       "378       []           0  \n",
       "416       []           0  \n",
       "417       []           0  \n",
       "510       []           0  \n",
       "522       []           0  \n",
       "565       []           0  \n",
       "587       []           0  \n",
       "627       []           0  \n",
       "633       []           0  \n",
       "635       []           0  \n",
       "668       []           0  \n",
       "679       []           0  \n",
       "...      ...         ...  \n",
       "13556     []           0  \n",
       "13560     []           0  \n",
       "13603     []           0  \n",
       "13609     []           0  \n",
       "13615     []           0  \n",
       "13628     []           0  \n",
       "13728     []           0  \n",
       "13778     []           0  \n",
       "13781     []           0  \n",
       "13787     []           0  \n",
       "13806     []           0  \n",
       "13814     []           0  \n",
       "13820     []           0  \n",
       "13825     []           0  \n",
       "13896     []           0  \n",
       "13904     []           0  \n",
       "13927     []           0  \n",
       "13938     []           0  \n",
       "13941     []           0  \n",
       "13958     []           0  \n",
       "13975     []           0  \n",
       "13991     []           0  \n",
       "14012     []           0  \n",
       "14015     []           0  \n",
       "14020     []           0  \n",
       "14024     []           0  \n",
       "14036     []           0  \n",
       "14037     []           0  \n",
       "14059     []           0  \n",
       "14080     []           0  \n",
       "\n",
       "[718 rows x 19 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df[movies_df.Len_Lemmas < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " #yeeshh, 718 not included???\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Text_No_Punc'] = movies_df.Text.map(lambda x: re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '',x))\n",
    "#this gets rid of apostrophes too, which may be problematic...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slave in the magic mirror come from the farthest space through wind and darkness i summon thee speak  let me see thy face '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df['Text_No_Punc'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Lemmas'] = movies_df.Text_No_Punc.map(lambda x: [wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slave', 'magic', 'mirror', 'come', 'farthest', 'space', 'wind', 'darkness', 'summon', 'let', 'see', 'thy', 'face'] \n",
      "\n",
      "['wouldst', 'know', 'queen'] \n",
      "\n",
      "['magic', 'mirror', 'wall', 'be', 'fairest'] \n",
      "\n",
      "['fame', 'be', 'thy', 'beauty', 'majesty', 'hold', 'lovely', 'maid', 'see', 'rag', 'hide', 'gentle', 'grace', 'be', 'more', 'fair'] \n",
      "\n",
      "['reveal', 'name'] \n",
      "\n",
      "['lip', 'red', 'rise', 'hair', 'black', 'ebony', 'skin', 'white', 'snow'] \n",
      "\n",
      "['snow', 'white'] \n",
      "\n",
      "['today'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 11):\n",
    "    print(movies_df.Lemmas.iloc[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     slave in the magic mirror come from the farthe...\n",
       "1                     what wouldst thou know my queen  \n",
       "2     magic mirror on the wall who is the fairest on...\n",
       "3     famed is thy beauty majesty but hold a lovely ...\n",
       "4                        alas for her  reveal her name \n",
       "5     lips red as the rose hair black as ebony skin ...\n",
       "6                                          snow white  \n",
       "8                                                today \n",
       "9                                                  oh  \n",
       "10                                               hello \n",
       "11                                                  oh \n",
       "Name: Text_No_Punc, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Text_No_Punc[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's try another method....\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cassi\\Desktop\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-4202e797c4d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmovies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m         \u001b[0mforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mapply_rules\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m             return [form[:-len(old)] + new\n\u001b[1;32m-> 1823\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m                     if form.endswith(old)]\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1823\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1824\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1825\u001b[1;33m                     if form.endswith(old)]\n\u001b[0m\u001b[0;32m   1826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1827\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "[lemmatizer.lemmatize((w, get_wordnet_pos(w)) for w in movies_df.Tokens.iloc[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features = 1500, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Lemmas'] = movies_df.Lemmas.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slave magic mirror come farthest space wind da...\n",
       "1                                   wouldst know queen\n",
       "2                         magic mirror wall be fairest\n",
       "3    fame be thy beauty majesty hold lovely maid se...\n",
       "4                                          reveal name\n",
       "Name: Lemmas, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_tfidf = tfidf_vectorizer.fit_transform(movies_df['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_topics(model, tfidf_vectorizer, n_top_words):\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Picking number of topics and number of words\n",
    "number_topics = 10\n",
    "number_words = 5\n",
    "\n",
    "#lda = LDA(n_components=number_topics)\n",
    "#lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "let really say talk ow\n",
      "\n",
      "Topic #1:\n",
      "come tell dragon hes true\n",
      "\n",
      "Topic #2:\n",
      "boy love whoa make good\n",
      "\n",
      "Topic #3:\n",
      "yes okay sorry sir jack\n",
      "\n",
      "Topic #4:\n",
      "im think want work sure\n",
      "\n",
      "Topic #5:\n",
      "know dad youre hold dont\n",
      "\n",
      "Topic #6:\n",
      "stop help just guy listen\n",
      "\n",
      "Topic #7:\n",
      "look dont thank shrek hiccup\n",
      "\n",
      "Topic #8:\n",
      "right way time mother ready\n",
      "\n",
      "Topic #9:\n",
      "princess sh believe new little\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "let really say talk ow dear wait prince eat leave\n",
      "\n",
      "Topic #1:\n",
      "come tell dragon hes true heart didnt die minute warrior\n",
      "\n",
      "Topic #2:\n",
      "boy love whoa make good girl mean bring watch doesnt\n",
      "\n",
      "Topic #3:\n",
      "yes okay sorry sir jack dream im happen hear nice\n",
      "\n",
      "Topic #4:\n",
      "im think want work sure night donkey ive gonna kid\n",
      "\n",
      "Topic #5:\n",
      "know dad youre hold dont mom head rapunzel away toothless\n",
      "\n",
      "Topic #6:\n",
      "stop help just guy listen master theyre bad kill tooth\n",
      "\n",
      "Topic #7:\n",
      "look dont thank shrek hiccup course stay like understand don\n",
      "\n",
      "Topic #8:\n",
      "right way time mother ready run better home maybe ya\n",
      "\n",
      "Topic #9:\n",
      "princess sh believe new little man care baby child big\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_movies = movies_df[movies_df.Gender == 'f']\n",
    "m_movies = movies_df[movies_df.Gender == 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_movies_tfidf = tfidf_vectorizer.fit_transform(f_movies['Lemmas'])\n",
    "m_movies_tfidf = tfidf_vectorizer.fit_transform(m_movies['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(f_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "imperial sayin highness joke child\n",
      "\n",
      "Topic #1:\n",
      "message play rid hair attention\n",
      "\n",
      "Topic #2:\n",
      "closer pool swamp smart training\n",
      "\n",
      "Topic #3:\n",
      "soon better attack object evangeline\n",
      "\n",
      "Topic #4:\n",
      "rain date lefou couldve got\n",
      "\n",
      "Topic #5:\n",
      "lawrence shell honor service brimstone\n",
      "\n",
      "Topic #6:\n",
      "tour meantime goodness crown lord\n",
      "\n",
      "Topic #7:\n",
      "need list fat hmm peculiar\n",
      "\n",
      "Topic #8:\n",
      "solid soap mad thursday irresponsible\n",
      "\n",
      "Topic #9:\n",
      "dog privilege kiss lie sooner\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(m_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "really majesty donkey course ariel\n",
      "\n",
      "Topic #1:\n",
      "im right hiccup didnt bud\n",
      "\n",
      "Topic #2:\n",
      "look thank way just dragon\n",
      "\n",
      "Topic #3:\n",
      "work hes run jack toothless\n",
      "\n",
      "Topic #4:\n",
      "yes okay time wait tell\n",
      "\n",
      "Topic #5:\n",
      "let sorry sir ow mean\n",
      "\n",
      "Topic #6:\n",
      "know come dont say youre\n",
      "\n",
      "Topic #7:\n",
      "dad hold sh talk ready\n",
      "\n",
      "Topic #8:\n",
      "stop help try make friend\n",
      "\n",
      "Topic #9:\n",
      "good whoa love bad princess\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay, topic 9 is kind of hilarious...\n",
    "#let's reduce the number of topics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_topics(model, tfidf_vectorizer, n_top_words):\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Picking number of topics and number of words\n",
    "number_topics = 5\n",
    "number_words = 5\n",
    "\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "crab pool mate shed mighty\n",
      "\n",
      "Topic #1:\n",
      "eric tooth know throne snack\n",
      "\n",
      "Topic #2:\n",
      "ruin survival report warren deserve\n",
      "\n",
      "Topic #3:\n",
      "scrub stop way know invite\n",
      "\n",
      "Topic #4:\n",
      "youd lower welcome incredible time\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use the raw text...\n",
    "movies_tfidf = tfidf_vectorizer.fit_transform(movies_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_topics = 5\n",
    "number_words = 5\n",
    "\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "uh stop good ve princess\n",
      "\n",
      "Topic #1:\n",
      "don yes know let ll\n",
      "\n",
      "Topic #2:\n",
      "come ah help look shrek\n",
      "\n",
      "Topic #3:\n",
      "hey wait love sorry just\n",
      "\n",
      "Topic #4:\n",
      "oh did yeah okay going\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_movies_tfidf = tfidf_vectorizer.fit_transform(f_movies['Text'])\n",
    "m_movies_tfidf = tfidf_vectorizer.fit_transform(m_movies['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(f_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "news large hands protect thanks\n",
      "\n",
      "Topic #1:\n",
      "yow dumped left prisoner shadow\n",
      "\n",
      "Topic #2:\n",
      "listen ll mighty step snow\n",
      "\n",
      "Topic #3:\n",
      "nest door waitress miss pig\n",
      "\n",
      "Topic #4:\n",
      "reach company looking dumb think\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(m_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "yeah come let wait got\n",
      "\n",
      "Topic #1:\n",
      "oh yes really sorry think\n",
      "\n",
      "Topic #2:\n",
      "look know hey don like\n",
      "\n",
      "Topic #3:\n",
      "okay did uh stop going\n",
      "\n",
      "Topic #4:\n",
      "ll huh doing dragon love\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Across roles?\n",
    "pro_movies = movies_df[movies_df.Role == 'PRO']\n",
    "ant_movies = movies_df[movies_df.Role == 'ANT']\n",
    "help_movies = movies_df[movies_df.Role == 'HELPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_movies_tfidf = tfidf_vectorizer.fit_transform(pro_movies['Lemmas'])\n",
    "ant_movies_tfidf = tfidf_vectorizer.fit_transform(ant_movies['Lemmas'])\n",
    "help_movies_tfidf = tfidf_vectorizer.fit_transform(help_movies['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(pro_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO Topics found via LDA:\n",
      "\n",
      "Topic #0:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-33f6e4a3339e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PRO Topics found via LDA:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-73-1c0322f8d1b8>\u001b[0m in \u001b[0;36mprint_topics\u001b[1;34m(model, tfidf_vectorizer, n_top_words)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #%d:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         print(\" \".join([words[i]\n\u001b[1;32m----> 6\u001b[1;33m                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Picking number of topics and number of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-1c0322f8d1b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #%d:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         print(\" \".join([words[i]\n\u001b[1;32m----> 6\u001b[1;33m                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Picking number of topics and number of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"PRO Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(ant_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "brain reason cover kingdom hard\n",
      "\n",
      "Topic #1:\n",
      "laughter guide look official block\n",
      "\n",
      "Topic #2:\n",
      "genius makin grieve plague piece\n",
      "\n",
      "Topic #3:\n",
      "scare globe scroll kiss comfortable\n",
      "\n",
      "Topic #4:\n",
      "buddy honest pass grumpy friend\n"
     ]
    }
   ],
   "source": [
    "print(\"PRO Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Attempt 2\n",
    "Maybe lines being so short might influence these topics? What if we try going by an entire pool of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_char_df = pd.read_pickle(r'C:/Users/cassi/Desktop/Data_Science/Animated-Movie-Gendered-Dialogue/private/char_toks.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 632 entries, 0 to 631\n",
      "Data columns (total 11 columns):\n",
      "Disney_Period       632 non-null object\n",
      "Gender              632 non-null object\n",
      "Movie               632 non-null object\n",
      "Role                632 non-null object\n",
      "Speaker             632 non-null object\n",
      "Speaker_Status      632 non-null object\n",
      "Total_Tok_Count     632 non-null float64\n",
      "Total_Toks          632 non-null object\n",
      "Total_Type_Count    632 non-null float64\n",
      "Total_Types         632 non-null object\n",
      "Year                632 non-null object\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 32.1+ KB\n"
     ]
    }
   ],
   "source": [
    "movie_char_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slave', 'in', 'the', 'magic', 'mirror', 'come', 'from', 'the', 'farthest', 'space', 'through', 'wind', 'and', 'darkness', 'i', 'summon', 'thee', '.', 'speak', '!', 'let', 'me', 'see', 'thy', 'face', '.', 'magic', 'mirror', 'on', 'the', 'wall', ',', 'who', 'is', 'the', 'fairest', 'one', 'of', 'all', '?', 'alas', 'for', 'her', '!', 'reveal', 'her', 'name', '.', 'snow', 'white', '!', 'take', 'her', 'far', 'into', 'the', 'forest', '.', 'find', 'some', 'secluded', 'glade', 'where', 'she', 'can', 'pick', 'wildflowers', '.', 'and', 'there', ',', 'my', 'faithful', 'huntsman', ',', 'you', 'will', 'kill', 'her', '!', 'silence', '!', 'you', 'know', 'the', 'penalty', 'if', 'you', 'fail', '.', 'but', 'to', 'make', 'doubly', 'sure', '...', 'you', 'do', 'not', 'fail', ',', 'bring', 'back', 'her', 'heart', '...', 'magic', 'mirror', 'on', 'the', 'wall', ',', 'who', 'now', 'is', 'the', 'fairest', 'one', 'of', 'all', '?', 'over', 'the', 'seven', 'jewelled', 'hills', ',', 'beyond', 'the', 'seventh', 'fall', ',', 'in', 'the', 'cottage', 'of', 'the', 'seven', 'dwarfs', ',', 'dwells', 'snow', 'white', ',', 'fairest', 'one', 'of', 'all', '.', 'snow', 'white', 'lies', 'dead', 'in', 'the', 'forest', '.', 'the', 'huntsman', 'has', 'brought', 'me', 'proof', '.', 'behold', ',', 'her', 'heart', '.', 'the', 'heart', 'of', 'a', 'pig', '!', 'then', 'i', \"'ve\", 'been', 'tricked', '!', 'the', 'heart', 'of', 'a', 'pig', '!', 'the', 'blundering', 'fool', '!', 'i', \"'ll\", 'go', 'myself', 'to', 'the', 'dwarfs', \"'\", 'cottage', '...', 'in', 'a', 'disguise', 'so', 'complete', 'no', 'one', 'will', 'ever', 'suspect', '.', 'now', ',', 'a', 'formula', 'to', 'transform', 'my', 'beauty', 'into', 'ugliness', '.', 'change', 'my', 'queenly', 'raiment', 'to', 'a', 'peddler', \"'s\", 'cloak', '.', 'mummy', 'dustto', 'make', 'me', 'old', '.', 'to', 'shroud', 'my', 'clothes', ',', 'the', 'black', 'of', 'night', '.', 'to', 'age', 'my', 'voice', ',', 'an', 'old', 'hag', \"'s\", 'cackle', '.', 'to', 'whiten', 'my', 'hair', ',', 'a', 'scream', 'of', 'fright', '.', 'a', 'blast', 'of', 'wind', '...', 'to', 'fan', 'my', 'hate', '!', 'a', 'thunderbolt', '...', 'to', 'mix', 'it', 'well', '.', 'now', '...', 'begin', 'thy', 'magic', 'spell', '.', 'look', '!', 'my', 'hands', '!', 'my', 'voice', '!', 'my', 'voice', '!', 'a', 'perfect', 'disguise', '.', 'and', 'now', ',', 'a', 'special', '...', 'sort', 'of', 'death', 'for', 'one', 'so', 'fair', '.', 'what', 'shall', 'it', 'be', '?', 'ah', '!', 'a', 'poisoned', 'apple', '!', 'sleeping', 'death', '.', 'one', 'taste', 'of', 'the', 'poisoned', 'apple', '...', 'and', 'the', 'victim', \"'s\", 'eyes', 'will', 'close', 'forever', '...', 'in', 'the', 'sleeping', 'death', '.', 'yodel', 'holalaeeay', 'holalaeeay', 'holalaeeayeela', 'eeayeeleeay', 'holalaeeay', 'holalaeeay', 'holalaeeayeela', 'leeayleeoleeay', 'i', \"'d\", 'like', 'to', 'dance', 'and', 'tap', 'my', 'feet', ',', 'but', 'they', 'wo', \"n't\", 'keep', 'in', 'rhythm', 'you', 'see', ',', 'i', 'washed', \"'em\", 'both', 'today', 'and', 'i', 'ca', \"n't\", 'do', 'nothin', \"'\", 'with', \"'em\", 'hohum', ',', 'the', 'tune', 'is', 'dumb', 'the', 'words', 'do', \"n't\", 'mean', 'a', 'thing', 'is', \"n't\", 'this', 'a', 'silly', 'song', 'for', 'anyone', 'to', 'sing', 'i', 'oh', ',', 'ggosh', '!', 'i', 'chased', 'a', 'polecat', 'up', 'a', 'tree', 'way', 'out', 'upon', 'a', 'limb', 'and', 'when', 'he', 'got', 'the', 'best', 'of', 'me', 'i', 'got', 'the', 'worst', 'of', 'him', 'hohum', ',', 'the', 'tune', 'is', 'dumb', 'the', 'words', 'do', \"n't\", 'mean', 'a', 'thing', 'is', \"n't\", 'this', 'a', 'silly', 'song', 'for', 'anyone', 'to', 'sing', 'ahhh', 'dip', 'the', 'apple', 'in', 'the', 'brew', '.', 'let', 'the', 'sleeping', 'death', 'seep', 'through', '.', 'look', '!', 'on', 'the', 'skin', '!', 'the', 'symbol', 'of', 'what', 'lies', 'within', '.', 'now', ',', 'turn', 'red', 'to', 'tempt', 'snow', 'white', '.', 'to', 'make', 'her', 'hunger', 'for', 'a', 'bite', '.', 'have', 'a', 'bite', '?', 'it', \"'s\", 'not', 'for', 'you', '!', 'it', \"'s\", 'for', 'snow', 'white', '.', 'when', 'she', 'breaks', 'the', 'tender', 'peel', 'to', 'taste', 'the', 'apple', 'in', 'my', 'hand', ',', 'her', 'breath', 'will', 'still', ',', 'her', 'blood', 'congeal', '.', 'then', 'i', \"'ll\", 'be', 'fairest', 'in', 'the', 'land', '.', 'but', 'wait', '!', 'there', 'may', 'be', 'an', 'antidote', '.', 'nothing', 'must', 'be', 'overlooked', '.', 'ah', '!', 'here', 'it', 'is', '!', 'the', 'victim', 'of', 'the', 'sleeping', 'death', '...', 'can', 'be', 'revived', 'only', 'by', 'love', \"'s\", 'first', 'kiss', '.', 'love', \"'s\", 'first', 'kiss', '!', 'bah', '!', 'no', 'fear', 'of', 'that', '.', 'the', 'dwarfs', 'will', 'think', 'she', \"'s\", 'dead', '.', 'she', \"'ll\", 'be', 'buried', 'alive', '!', 'buried', 'alive', '!', 'thirsty', '?', 'have', 'a', 'drink', '!', 'the', 'little', 'men', 'will', 'be', 'away', '...', 'and', 'she', \"'ll\", 'be', 'alone', '...', 'with', 'a', 'harmless', 'old', 'peddler', 'woman', '.', 'a', 'harmless', 'old', 'peddler', 'woman', '.', 'all', 'alone', ',', 'my', 'pet', '?', 'the', '...', 'little', 'men', 'are', 'not', 'here', '?', 'makin', \"'\", 'pies', '?', 'it', \"'s\", 'apple', 'pies', 'that', 'makes', 'the', 'menfolks', \"'\", 'mouths', 'water', '.', 'pies', 'made', 'from', 'apples', 'like', 'these', '.', 'yes', '!', 'but', 'wait', \"'til\", 'you', 'taste', 'one', ',', 'dearie', '.', 'like', 'to', 'try', 'one', '?', 'go', 'on', '.', 'go', 'on', ',', 'have', 'a', 'bite', '.', 'ah', '!', 'ah', '!', 'oh', '!', 'my', 'heart', '!', 'oh', ',', 'mymy', 'poor', 'heart', '.', 'take', 'me', 'into', 'the', 'house', 'and', 'let', 'me', 'rest', '.', 'a', 'drink', 'of', 'water', ',', 'please', '.', 'and', 'because', 'you', \"'ve\", 'been', 'so', 'good', 'to', 'poor', 'old', 'granny', ',', 'i', \"'ll\", 'share', 'a', 'secret', 'with', 'you', '.', 'this', 'is', 'no', 'ordinary', 'apple', '.', 'it', \"'s\", 'a', 'magic', 'wishing', 'apple', '.', 'yes', '!', 'one', 'bite', 'and', 'all', 'your', 'dreams', 'will', 'come', 'true', '.', 'yes', ',', 'girlie', '!', 'now', ',', 'make', 'a', 'wish', 'and', 'take', 'a', 'bite', '.', 'there', 'must', 'be', 'something', 'your', 'little', 'heart', 'desires', '.', 'perhaps', 'there', \"'s\", 'someone', 'you', 'love', '.', 'i', 'thought', 'so', '.', 'i', 'thought', 'so', '.', 'old', 'granny', 'knows', 'a', 'girl', \"'s\", 'heart', '.', 'now', ',', 'take', 'the', 'apple', ',', 'dearie', ',', 'and', 'make', 'a', 'wish', '.', 'that', \"'s\", 'it', '.', 'go', 'on', '.', 'go', 'on', '.', 'fine', '!', 'fine', '!', 'now', ',', 'take', 'a', 'bite', '.', 'do', \"n't\", 'let', 'the', 'wish', 'grow', 'cold', '!', 'her', 'breath', 'will', 'still', '.', 'her', 'blood', 'congeal', '.', 'now', 'i', \"'ll\", 'be', 'fairest', 'in', 'the', 'land', '!', 'i', \"'m\", 'trapped', '.', 'what', 'will', 'i', 'do', '?', 'the', 'meddling', 'little', 'fools', '!', 'i', \"'ll\", 'fix', 'ya', '!', 'i', \"'ll\", 'crush', 'your', 'bones', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_char_df.Total_Toks.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_char_df['Text'] = movie_char_df.Total_Toks.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"slave in the magic mirror come from the farthest space through wind and darkness i summon thee . speak ! let me see thy face . magic mirror on the wall , who is the fairest one of all ? alas for her ! reveal her name . snow white ! take her far into the forest . find some secluded glade where she can pick wildflowers . and there , my faithful huntsman , you will kill her ! silence ! you know the penalty if you fail . but to make doubly sure ... you do not fail , bring back her heart ... magic mirror on the wall , who now is the fairest one of all ? over the seven jewelled hills , beyond the seventh fall , in the cottage of the seven dwarfs , dwells snow white , fairest one of all . snow white lies dead in the forest . the huntsman has brought me proof . behold , her heart . the heart of a pig ! then i 've been tricked ! the heart of a pig ! the blundering fool ! i 'll go myself to the dwarfs ' cottage ... in a disguise so complete no one will ever suspect . now , a formula to transform my beauty into ugliness . change my queenly raiment to a peddler 's cloak . mummy dustto make me old . to shroud my clothes , the black of night . to age my voice , an old hag 's cackle . to whiten my hair , a scream of fright . a blast of wind ... to fan my hate ! a thunderbolt ... to mix it well . now ... begin thy magic spell . look ! my hands ! my voice ! my voice ! a perfect disguise . and now , a special ... sort of death for one so fair . what shall it be ? ah ! a poisoned apple ! sleeping death . one taste of the poisoned apple ... and the victim 's eyes will close forever ... in the sleeping death . yodel holalaeeay holalaeeay holalaeeayeela eeayeeleeay holalaeeay holalaeeay holalaeeayeela leeayleeoleeay i 'd like to dance and tap my feet , but they wo n't keep in rhythm you see , i washed 'em both today and i ca n't do nothin ' with 'em hohum , the tune is dumb the words do n't mean a thing is n't this a silly song for anyone to sing i oh , ggosh ! i chased a polecat up a tree way out upon a limb and when he got the best of me i got the worst of him hohum , the tune is dumb the words do n't mean a thing is n't this a silly song for anyone to sing ahhh dip the apple in the brew . let the sleeping death seep through . look ! on the skin ! the symbol of what lies within . now , turn red to tempt snow white . to make her hunger for a bite . have a bite ? it 's not for you ! it 's for snow white . when she breaks the tender peel to taste the apple in my hand , her breath will still , her blood congeal . then i 'll be fairest in the land . but wait ! there may be an antidote . nothing must be overlooked . ah ! here it is ! the victim of the sleeping death ... can be revived only by love 's first kiss . love 's first kiss ! bah ! no fear of that . the dwarfs will think she 's dead . she 'll be buried alive ! buried alive ! thirsty ? have a drink ! the little men will be away ... and she 'll be alone ... with a harmless old peddler woman . a harmless old peddler woman . all alone , my pet ? the ... little men are not here ? makin ' pies ? it 's apple pies that makes the menfolks ' mouths water . pies made from apples like these . yes ! but wait 'til you taste one , dearie . like to try one ? go on . go on , have a bite . ah ! ah ! oh ! my heart ! oh , mymy poor heart . take me into the house and let me rest . a drink of water , please . and because you 've been so good to poor old granny , i 'll share a secret with you . this is no ordinary apple . it 's a magic wishing apple . yes ! one bite and all your dreams will come true . yes , girlie ! now , make a wish and take a bite . there must be something your little heart desires . perhaps there 's someone you love . i thought so . i thought so . old granny knows a girl 's heart . now , take the apple , dearie , and make a wish . that 's it . go on . go on . fine ! fine ! now , take a bite . do n't let the wish grow cold ! her breath will still . her blood congeal . now i 'll be fairest in the land ! i 'm trapped . what will i do ? the meddling little fools ! i 'll fix ya ! i 'll crush your bones !\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_char_df['Text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = movie_char_df['Text'].iloc[0]\n",
    "lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slave', 'magic', 'mirror', 'come', 'farthest', 'space', 'wind', 'darkness', 'summon', 'let', 'see', 'thy', 'face', 'magic', 'mirror', 'wall', 'be', 'fairest', 'reveal', 'name', 'snow', 'white', 'take', 'far', 'forest', 'find', 'seclude', 'glade', 'pick', 'wildflower', 'faithful', 'huntsman', 'kill', 'silence', 'know', 'penalty', 'fail', 'make', 'doubly', 'sure', 'do', 'not', 'fail', 'bring', 'heart', 'magic', 'mirror', 'wall', 'now', 'be', 'fairest', 'jewelled', 'hill', 'seventh', 'fall', 'cottage', 'dwarf', 'dwell', 'snow', 'white', 'fairest', 'snow', 'white', 'lie', 'dead', 'forest', 'huntsman', 'have', 'bring', 'proof', 'behold', 'heart', 'heart', 'pig', 'then', 've', 'be', 'trick', 'heart', 'pig', 'blundering', 'fool', 'll', 'go', 'dwarf', 'cottage', 'disguise', 'so', 'complete', 'no', 'one', 'ever', 'suspect', 'now', 'formula', 'transform', 'beauty', 'ugliness', 'change', 'queenly', 'raiment', 'peddler', 'cloak', 'mummy', 'make', 'old', 'shroud', 'clothe', 'black', 'night', 'age', 'voice', 'old', 'hag', 'cackle', 'whiten', 'hair', 'scream', 'fright', 'blast', 'wind', 'fan', 'hate', 'thunderbolt', 'mix', 'well', 'now', 'begin', 'thy', 'spell', 'look', 'hand', 'voice', 'voice', 'perfect', 'disguise', 'now', 'special', 'sort', 'death', 'so', 'fair', 'be', 'poison', 'apple', 'sleep', 'death', 'taste', 'poison', 'apple', 'victim', 'eye', 'close', 'forever', 'sleeping', 'death', 'yodel', 'holalaeeay', 'holalaeeay', 'holalaeeayeela', 'eeayeeleeay', 'holalaeeay', 'holalaeeay', 'holalaeeayeela', 'leeayleeoleeay', 'dance', 'tap', 'foot', 'keep', 'rhythm', 'see', 'wash', 'today', 'do', 'nothin', 'hohum', 'tune', 'be', 'dumb', 'word', 'do', 'mean', 'thing', 'be', 'silly', 'song', 'anyone', 'sing', 'ggosh', 'chase', 'polecat', 'tree', 'way', 'limb', 'get', 'best', 'get', 'worst', 'hohum', 'tune', 'be', 'dumb', 'word', 'do', 'mean', 'thing', 'be', 'silly', 'song', 'anyone', 'sing', 'ahhh', 'dip', 'apple', 'brew', 'let', 'sleeping', 'death', 'seep', 'look', 'skin', 'symbol', 'lie', 'now', 'turn', 'red', 'tempt', 'snow', 'white', 'make', 'hunger', 'bite', 'have', 'bite', 'not', 'snow', 'white', 'break', 'tender', 'peel', 'taste', 'apple', 'hand', 'breath', 'still', 'blood', 'congeal', 'then', 'be', 'fairest', 'land', 'be', 'antidote', 'nothing', 'be', 'overlooked', 'here', 'be', 'victim', 'sleeping', 'death', 'be', 'revive', 'only', 'love', 'first', 'kiss', 'love', 'first', 'kiss', 'bah', 'fear', 'dwarf', 'think', 'dead', 'll', 'be', 'bury', 'alive', 'buried', 'alive', 'thirsty', 'have', 'drink', 'little', 'man', 'be', 'away', 'll', 'be', 'alone', 'harmless', 'old', 'peddler', 'woman', 'harmless', 'old', 'peddler', 'woman', 'alone', 'pet', 'little', 'man', 'be', 'not', 'here', 'makin', 'py', 'apple', 'py', 'make', 'mouth', 'water', 'py', 'make', 'apple', 'ye', 'wait', 'taste', 'dearie', 'try', 'go', 'go', 'have', 'bite', 'heart', 'mymy', 'poor', 'heart', 'take', 'house', 'let', 'rest', 'drink', 'water', 'please', 'have', 'be', 'so', 'good', 'poor', 'old', 'granny', 'll', 'share', 'secret', 'be', 'no', 'ordinary', 'apple', 'magic', 'wish', 'apple', 'yes', 'bite', 'dream', 'come', 'true', 'yes', 'girlie', 'now', 'make', 'wish', 'take', 'bite', 'be', 'something', 'little', 'heart', 'desire', 'perhaps', 'someone', 'love', 'think', 'think', 'so', 'old', 'granny', 'know', 'girl', 'heart', 'now', 'take', 'apple', 'dearie', 'make', 'wish', 'go', 'go', 'fine', 'fine', 'now', 'take', 'bite', 'do', 'let', 'wish', 'grow', 'cold', 'breath', 'still', 'blood', 'congeal', 'now', 'be', 'fairest', 'land', 'trap', 'do', 'meddling', 'little', 'fool', 'll', 'fix', 'ya', 'll', 'crush', 'bone']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize all\n",
    "movie_char_df['Lemmas'] = movie_char_df.Text.map(lambda x: [wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_chars = movie_char_df[movie_char_df.Gender == 'f']\n",
    "m_chars = movie_char_df[movie_char_df.Gender == 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features = 1500, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_char_df['Lemmas'] = movie_char_df.Lemmas.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slave magic mirror come farthest space wind da...\n",
       "1    wouldst know queen fame be thy beauty majesty ...\n",
       "2                                           today song\n",
       "3    matter mama papa believe re lose please do com...\n",
       "4    yes majesty majesty little princess majesty do...\n",
       "Name: Lemmas, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_char_df.Lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tfidf = tfidf_vectorizer.fit_transform(movie_char_df['Lemmas'])\n",
    "m_tfidf = tfidf_vectorizer.fit_transform(movie_char_df['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, tfidf_vectorizer, n_top_words):\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Picking number of topics and number of words\n",
    "number_topics = 5\n",
    "number_words = 5\n",
    "\n",
    "#lda = LDA(n_components=number_topics)\n",
    "#lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(f_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "worker bala dig arendelle princess\n",
      "\n",
      "Topic #1:\n",
      "come know just ll let\n",
      "\n",
      "Topic #2:\n",
      "moana da dream bark hurrah\n",
      "\n",
      "Topic #3:\n",
      "open cinderella metro jack mate\n",
      "\n",
      "Topic #4:\n",
      "sir princess girl puppet maurice\n"
     ]
    }
   ],
   "source": [
    "print(\"Female Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(m_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "come know ll just look\n",
      "\n",
      "Topic #1:\n",
      "worker arendelle da majesty jamie\n",
      "\n",
      "Topic #2:\n",
      "girl emperor eric come careful\n",
      "\n",
      "Topic #3:\n",
      "morning ant snow pretty hurrah\n",
      "\n",
      "Topic #4:\n",
      "princess mulan sir open nice\n"
     ]
    }
   ],
   "source": [
    "print(\"Male Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Across roles?\n",
    "pro_chars = movie_char_df[movie_char_df.Role == 'PRO']\n",
    "ant_chars = movie_char_df[movie_char_df.Role == 'ANT']\n",
    "help_chars = movie_char_df[movie_char_df.Role == 'HELPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_tfidf = tfidf_vectorizer.fit_transform(pro_chars['Lemmas'])\n",
    "ant_tfidf = tfidf_vectorizer.fit_transform(ant_chars['Lemmas'])\n",
    "help_tfidf = tfidf_vectorizer.fit_transform(help_chars['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(pro_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pro Topics found via LDA:\n",
      "\n",
      "Topic #0:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-b244f5ba82bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pro Topics found via LDA:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-6ad394599067>\u001b[0m in \u001b[0;36mprint_topics\u001b[1;34m(model, tfidf_vectorizer, n_top_words)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #%d:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         print(\" \".join([words[i]\n\u001b[1;32m----> 6\u001b[1;33m                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Picking number of topics and number of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-6ad394599067>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #%d:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         print(\" \".join([words[i]\n\u001b[1;32m----> 6\u001b[1;33m                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Picking number of topics and number of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Pro Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANT Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "cake answer po okay pull\n",
      "\n",
      "Topic #1:\n",
      "flower deny nice fool answer\n",
      "\n",
      "Topic #2:\n",
      "cookies height nose minute island\n",
      "\n",
      "Topic #3:\n",
      "clear pocahontas fear fine babe\n",
      "\n",
      "Topic #4:\n",
      "git friend forget gift boring\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(ant_tfidf)\n",
    "\n",
    "print(\"ANT Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELPER Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "cinderelly shh dear slipper oooh\n",
      "\n",
      "Topic #1:\n",
      "ariel dragon ll ya tell\n",
      "\n",
      "Topic #2:\n",
      "dragon master watch mama da\n",
      "\n",
      "Topic #3:\n",
      "know shrek ll think come\n",
      "\n",
      "Topic #4:\n",
      "master yah believe jack wish\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(help_tfidf)\n",
    "\n",
    "print(\"HELPER Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
