{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis\n",
    "Do characters of different genders talk about different things? In this code I will...\n",
    "\n",
    "1) Lemmatize each line of text\n",
    "\n",
    "2) Use these lemmas to create topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary stuff\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_pickle(r\"C:/Users/cassi/Desktop/Data_Science/Animated-Movie-Gendered-Dialogue/private/all_tagged_dialogue.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13442 entries, 0 to 14095\n",
      "Data columns (total 17 columns):\n",
      "Disney_Period       13442 non-null object\n",
      "Gender              13442 non-null object\n",
      "Movie               13442 non-null object\n",
      "Role                13442 non-null object\n",
      "Song                13442 non-null object\n",
      "Speaker             13442 non-null object\n",
      "Speaker_Status      13442 non-null object\n",
      "Text                13442 non-null object\n",
      "UTTERANCE_NUMBER    13442 non-null int64\n",
      "Year                13442 non-null int64\n",
      "Tokens              13442 non-null object\n",
      "Types               13442 non-null object\n",
      "Token_Count         13442 non-null int64\n",
      "Type_Count          13442 non-null int64\n",
      "POS                 13442 non-null object\n",
      "Tag_Freq            13442 non-null object\n",
      "Command_Count       13442 non-null int64\n",
      "dtypes: int64(5), object(12)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "NLTK offers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slave in the magic mirror come from the farthest space through wind and darkness i summon thee. speak ! let me see thy face. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = movies_df.Text.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmmm. Well, I've tried installing pattern but without much success. Let's try another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slave',\n",
       " 'magic',\n",
       " 'mirror',\n",
       " 'come',\n",
       " 'farthest',\n",
       " 'space',\n",
       " 'wind',\n",
       " 'darkness',\n",
       " 'summon',\n",
       " 'let',\n",
       " 'see',\n",
       " 'thy',\n",
       " 'face']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = []\n",
    "for i in range(0,11):\n",
    "    sent_list.append(movies_df.Text.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas=[]\n",
    "for sent in sent_list:\n",
    "    lemmas.append([wd.decode('utf-8').split('/')[0] for wd in lemmatize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slave', 'magic', 'mirror', 'come', 'farthest', 'space', 'wind', 'darkness', 'summon', 'let', 'see', 'thy', 'face'] \n",
      "\n",
      "['wouldst', 'know', 'queen'] \n",
      "\n",
      "['magic', 'mirror', 'wall', 'be', 'fairest'] \n",
      "\n",
      "['fame', 'be', 'thy', 'beauty', 'majesty', 'hold', 'lovely', 'maid', 'see', 'rag', 'hide', 'gentle', 'grace', 'be', 'more', 'fair'] \n",
      "\n",
      "['reveal', 'name'] \n",
      "\n",
      "['lip', 'red', 'rise', 'hair', 'black', 'ebony', 'skin', 'white', 'snow'] \n",
      "\n",
      "['snow', 'white'] \n",
      "\n",
      "['today'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in lemmas:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper as ttw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     slave in the magic mirror come from the farthe...\n",
       "1                   what wouldst thou know, my queen ? \n",
       "2     magic mirror on the wall, who is the fairest o...\n",
       "3     famed is thy beauty, majesty. but hold, a love...\n",
       "4                      alas for her ! reveal her name. \n",
       "5     lips red as the rose. hair black as ebony. ski...\n",
       "6                                         snow white ! \n",
       "8                                                today \n",
       "9                                                 oh ! \n",
       "10                                              hello. \n",
       "11                                                 oh. \n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Text.iloc[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#remove punctuation maybe?\n",
    "symbols = \".!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TreeTaggerError",
     "evalue": "Can't locate TreeTagger directory (and no TAGDIR specified).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTreeTaggerError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6d820233ccf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtagger\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mttw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTAGLANG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\treetaggerwrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using treetaggerwrapper.py from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Note: TreeTagger process is started later, when really needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\treetaggerwrapper.py\u001b[0m in \u001b[0;36m_set_tagger\u001b[1;34m(self, kargs)\u001b[0m\n\u001b[0;32m   1046\u001b[0m                 logger.error(\"Can't locate TreeTagger directory (and \"\n\u001b[0;32m   1047\u001b[0m                              \"no TAGDIR specified).\")\n\u001b[1;32m-> 1048\u001b[1;33m                 raise TreeTaggerError(\"Can't locate TreeTagger directory (and \"\n\u001b[0m\u001b[0;32m   1049\u001b[0m                                       \"no TAGDIR specified).\")\n\u001b[0;32m   1050\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTreeTaggerError\u001b[0m: Can't locate TreeTagger directory (and no TAGDIR specified)."
     ]
    }
   ],
   "source": [
    "tagger  = ttw.TreeTagger(TAGLANG = 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can experiment with more lemmatizers later. For now, let's keep going with gensim's LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Lemmas'] = movies_df.Text.map(lambda x: [wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [slave, magic, mirror, come, farthest, space, ...\n",
       "1                                   [wouldst, know, queen]\n",
       "2                       [magic, mirror, wall, be, fairest]\n",
       "3        [fame, be, thy, beauty, majesty, hold, lovely,...\n",
       "4                                           [reveal, name]\n",
       "5        [lip, red, rise, hair, black, ebony, skin, whi...\n",
       "6                                            [snow, white]\n",
       "8                                                  [today]\n",
       "9                                                       []\n",
       "10                                                      []\n",
       "11                                                      []\n",
       "13       [take, far, forest, find, seclude, glade, pick...\n",
       "14                                          [yes, majesty]\n",
       "15                              [faithful, huntsman, kill]\n",
       "16                             [majesty, little, princess]\n",
       "17                                   [know, penalty, fail]\n",
       "18                                          [yes, majesty]\n",
       "19       [make, doubly, sure, do, not, fail, bring, heart]\n",
       "21       [matter, mama, papa, believe, re, lose, please...\n",
       "22                   [do, forgive, beg, highness, forgive]\n",
       "23                                       [don, understand]\n",
       "24                       [mad, jealous, ll, stop, nothing]\n",
       "25                                                      []\n",
       "26                                                 [queen]\n",
       "27                                                 [queen]\n",
       "28       [now, quick, child, run, run, away, hide, wood...\n",
       "29       [please, don, run, away, win, hurt, awfully, s...\n",
       "34       [really, feel, quite, happy, now, sure, ll, ge...\n",
       "35       [sleep, ground, tree, way, do, sure, nest, pos...\n",
       "36                                                    [do]\n",
       "                               ...                        \n",
       "14066                                        [never, come]\n",
       "14067                                       [fight, blast]\n",
       "14068                                     [alpha, protect]\n",
       "14069                    [never, cease, amaze, bud, thank]\n",
       "14070                  [toothless, know, doesn, wash, out]\n",
       "14071                                           [stormfly]\n",
       "14072                               [give, cuddle, grumpy]\n",
       "14073                                   [little, princess]\n",
       "14074                                     [miss, so, much]\n",
       "14075                  [don, ever, leave, again, hookfang]\n",
       "14076                                               [barf]\n",
       "14077                                                [not]\n",
       "14078    [be, pretty, fine, dragon, wrangling, back, th...\n",
       "14079             [know, gonna, need, somebody, look, now]\n",
       "14080                                                   []\n",
       "14081                                          [be, honor]\n",
       "14082                         [father, be, bit, proud, be]\n",
       "14083                 [thank, really, glad, re, here, mom]\n",
       "14084                                     [here, ll, stay]\n",
       "14085                                [see, tell, be, here]\n",
       "14086                   [still, do, hilarious, come, here]\n",
       "14087                                                [ooh]\n",
       "14088                            [chief, have, come, home]\n",
       "14089    [be, berk, bit, trampled, busted, cover, ice, ...\n",
       "14090    [attack, be, relentless, crazy, stop, even, mo...\n",
       "14091    [be, small, number, stand, something, bigger, ...\n",
       "14092          [be, voice, peace, bit, bit, change, world]\n",
       "14093    [see, have, something, don, sure, have, army, ...\n",
       "14094                                               [have]\n",
       "14095                                             [dragon]\n",
       "Name: Lemmas, Length: 13442, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"Len_Lemmas\"] = movies_df.Lemmas.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disney_Period</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Role</th>\n",
       "      <th>Song</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speaker_Status</th>\n",
       "      <th>Text</th>\n",
       "      <th>UTTERANCE_NUMBER</th>\n",
       "      <th>Year</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Types</th>\n",
       "      <th>Token_Count</th>\n",
       "      <th>Type_Count</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag_Freq</th>\n",
       "      <th>Command_Count</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Len_Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh !</td>\n",
       "      <td>10</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, !]</td>\n",
       "      <td>{!, oh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (!, .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>prince</td>\n",
       "      <td>PRINCE</td>\n",
       "      <td>hello.</td>\n",
       "      <td>11</td>\n",
       "      <td>1937</td>\n",
       "      <td>[hello, .]</td>\n",
       "      <td>{., hello}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(hello, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh.</td>\n",
       "      <td>12</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, .]</td>\n",
       "      <td>{oh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (., .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>butbut who ?</td>\n",
       "      <td>26</td>\n",
       "      <td>1937</td>\n",
       "      <td>[butbut, who, ?]</td>\n",
       "      <td>{butbut, ?, who}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(butbut, NN), (who, WP), (?, .)]</td>\n",
       "      <td>{'NN': 1, 'WP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>happy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>gosh.</td>\n",
       "      <td>65</td>\n",
       "      <td>1937</td>\n",
       "      <td>[gosh, .]</td>\n",
       "      <td>{gosh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(gosh, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>sneezy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, ahahah</td>\n",
       "      <td>92</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, ahahah]</td>\n",
       "      <td>{oh, ahahah, ,}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(oh, UH), (,, ,), (ahahah, JJ)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'JJ': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>happy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>whawhat's that ? that's it.</td>\n",
       "      <td>96</td>\n",
       "      <td>1937</td>\n",
       "      <td>[whawhat, 's, that, ?, that, 's, it, .]</td>\n",
       "      <td>{whawhat, 's, ?, ., that, it}</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>[(whawhat, WP), ('s, VBZ), (that, DT), (?, .),...</td>\n",
       "      <td>{'WP': 1, 'VBZ': 2, 'DT': 2, '.': 2, 'PRP': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>doc</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>it's up there.</td>\n",
       "      <td>98</td>\n",
       "      <td>1937</td>\n",
       "      <td>[it, 's, up, there, .]</td>\n",
       "      <td>{'s, ., there, up, it}</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[(it, PRP), ('s, VBZ), (up, RP), (there, RB), ...</td>\n",
       "      <td>{'PRP': 1, 'VBZ': 1, 'RP': 1, 'RB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>bashful</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, gosh !</td>\n",
       "      <td>136</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, gosh, !]</td>\n",
       "      <td>{gosh, !, oh, ,}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (gosh, JJ), (!, .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'JJ': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>and you ?</td>\n",
       "      <td>139</td>\n",
       "      <td>1937</td>\n",
       "      <td>[and, you, ?]</td>\n",
       "      <td>{you, ?, and}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(and, CC), (you, PRP), (?, .)]</td>\n",
       "      <td>{'CC': 1, 'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>doc</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, yeah.</td>\n",
       "      <td>149</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, yeah, .]</td>\n",
       "      <td>{oh, ., yeah, ,}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (yeah, UH), (., .)]</td>\n",
       "      <td>{'UH': 2, ',': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>sneezy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>hey ! hey ! hey ! hey !</td>\n",
       "      <td>240</td>\n",
       "      <td>1937</td>\n",
       "      <td>[hey, !, hey, !, hey, !, hey, !]</td>\n",
       "      <td>{!, hey}</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>[(hey, NN), (!, .), (hey, NN), (!, .), (hey, N...</td>\n",
       "      <td>{'NN': 4, '.': 4}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>sneezy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, gosh !</td>\n",
       "      <td>293</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, ,, gosh, !]</td>\n",
       "      <td>{gosh, !, oh, ,}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (gosh, JJ), (!, .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'JJ': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>grumpy</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>huh !</td>\n",
       "      <td>301</td>\n",
       "      <td>1937</td>\n",
       "      <td>[huh, !]</td>\n",
       "      <td>{!, huh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(huh, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>queen</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>ah ! ah !</td>\n",
       "      <td>318</td>\n",
       "      <td>1937</td>\n",
       "      <td>[ah, !, ah, !]</td>\n",
       "      <td>{!, ah}</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[(ah, NN), (!, .), (ah, NN), (!, .)]</td>\n",
       "      <td>{'NN': 2, '.': 2}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh.</td>\n",
       "      <td>353</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, .]</td>\n",
       "      <td>{oh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (., .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Snow White</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>snow white</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh.</td>\n",
       "      <td>355</td>\n",
       "      <td>1937</td>\n",
       "      <td>[oh, .]</td>\n",
       "      <td>{oh, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (., .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>birds chirping</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>why?</td>\n",
       "      <td>7</td>\n",
       "      <td>1950</td>\n",
       "      <td>[why, ?]</td>\n",
       "      <td>{why, ?}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(why, WRB), (?, .)]</td>\n",
       "      <td>{'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>mice</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>he!</td>\n",
       "      <td>16</td>\n",
       "      <td>1950</td>\n",
       "      <td>[he, !]</td>\n",
       "      <td>{he, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(he, PRP), (!, .)]</td>\n",
       "      <td>{'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>mice</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>up!</td>\n",
       "      <td>54</td>\n",
       "      <td>1950</td>\n",
       "      <td>[up, !]</td>\n",
       "      <td>{up, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(up, RB), (!, .)]</td>\n",
       "      <td>{'RB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>gus</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>up!</td>\n",
       "      <td>55</td>\n",
       "      <td>1950</td>\n",
       "      <td>[up, !]</td>\n",
       "      <td>{up, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(up, RB), (!, .)]</td>\n",
       "      <td>{'RB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drizella</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>why, that's us!</td>\n",
       "      <td>148</td>\n",
       "      <td>1950</td>\n",
       "      <td>[why, ,, that, 's, us, !]</td>\n",
       "      <td>{!, 's, us, that, why, ,}</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[(why, WRB), (,, ,), (that, DT), ('s, VBZ), (u...</td>\n",
       "      <td>{'WRB': 1, ',': 1, 'DT': 1, 'VBZ': 1, 'PRP': 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drizella</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh \"if\"</td>\n",
       "      <td>160</td>\n",
       "      <td>1950</td>\n",
       "      <td>[oh, ``, if, '']</td>\n",
       "      <td>{if, oh, '', ``}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (``, ``), (if, IN), ('', '')]</td>\n",
       "      <td>{'UH': 1, '``': 1, 'IN': 1, '''': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>mouse</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh!</td>\n",
       "      <td>203</td>\n",
       "      <td>1950</td>\n",
       "      <td>[oh, !]</td>\n",
       "      <td>{!, oh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(oh, UH), (!, .)]</td>\n",
       "      <td>{'UH': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>oh no!</td>\n",
       "      <td>225</td>\n",
       "      <td>1950</td>\n",
       "      <td>[oh, no, !]</td>\n",
       "      <td>{no, !, oh}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(oh, UH), (no, DT), (!, .)]</td>\n",
       "      <td>{'UH': 1, 'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>n</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>horse</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>ahem</td>\n",
       "      <td>265</td>\n",
       "      <td>1950</td>\n",
       "      <td>[ahem]</td>\n",
       "      <td>{ahem}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(ahem, NN)]</td>\n",
       "      <td>{'NN': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>another one?</td>\n",
       "      <td>271</td>\n",
       "      <td>1950</td>\n",
       "      <td>[another, one, ?]</td>\n",
       "      <td>{another, one, ?}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(another, DT), (one, CD), (?, .)]</td>\n",
       "      <td>{'DT': 1, 'CD': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>f</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>PRINCESS</td>\n",
       "      <td>but uh</td>\n",
       "      <td>273</td>\n",
       "      <td>1950</td>\n",
       "      <td>[but, uh]</td>\n",
       "      <td>{but, uh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(but, CC), (uh, NN)]</td>\n",
       "      <td>{'CC': 1, 'NN': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>grand duke</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>ahem.</td>\n",
       "      <td>306</td>\n",
       "      <td>1950</td>\n",
       "      <td>[ahem, .]</td>\n",
       "      <td>{ahem, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(ahem, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>EARLY</td>\n",
       "      <td>m</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>prince</td>\n",
       "      <td>PRINCE</td>\n",
       "      <td>but why?</td>\n",
       "      <td>317</td>\n",
       "      <td>1950</td>\n",
       "      <td>[but, why, ?]</td>\n",
       "      <td>{why, but, ?}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(but, CC), (why, WRB), (?, .)]</td>\n",
       "      <td>{'CC': 1, 'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>snotlout</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>yeah...</td>\n",
       "      <td>169</td>\n",
       "      <td>2014</td>\n",
       "      <td>[yeah, ...]</td>\n",
       "      <td>{..., yeah}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(yeah, NN), (..., :)]</td>\n",
       "      <td>{'NN': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13560</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what? why?!</td>\n",
       "      <td>173</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ?, why, ?, !]</td>\n",
       "      <td>{why, !, what, ?}</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[(what, WP), (?, .), (why, WRB), (?, .), (!, .)]</td>\n",
       "      <td>{'WP': 1, '.': 3, 'WRB': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>why?</td>\n",
       "      <td>216</td>\n",
       "      <td>2014</td>\n",
       "      <td>[why, ?]</td>\n",
       "      <td>{why, ?}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(why, WRB), (?, .)]</td>\n",
       "      <td>{'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13609</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>may i?</td>\n",
       "      <td>222</td>\n",
       "      <td>2014</td>\n",
       "      <td>[may, i, ?]</td>\n",
       "      <td>{i, ?, may}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(may, MD), (i, VB), (?, .)]</td>\n",
       "      <td>{'MD': 1, 'VB': 1, '.': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>ruffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, my.</td>\n",
       "      <td>228</td>\n",
       "      <td>2014</td>\n",
       "      <td>[oh, ,, my, .]</td>\n",
       "      <td>{my, oh, ., ,}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (my, PRP$), (., .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'PRP$': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13628</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no.</td>\n",
       "      <td>241</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, .]</td>\n",
       "      <td>{no, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (., .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13728</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>wow.</td>\n",
       "      <td>341</td>\n",
       "      <td>2014</td>\n",
       "      <td>[wow, .]</td>\n",
       "      <td>{., wow}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(wow, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13778</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>astrid</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what's down there?</td>\n",
       "      <td>391</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, 's, down, there, ?]</td>\n",
       "      <td>{'s, ?, down, there, what}</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[(what, WP), ('s, VBZ), (down, RB), (there, RB...</td>\n",
       "      <td>{'WP': 1, 'VBZ': 1, 'RB': 2, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13781</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>drago!</td>\n",
       "      <td>394</td>\n",
       "      <td>2014</td>\n",
       "      <td>[drago, !]</td>\n",
       "      <td>{!, drago}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(drago, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13787</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>ruffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>hey!</td>\n",
       "      <td>400</td>\n",
       "      <td>2014</td>\n",
       "      <td>[hey, !]</td>\n",
       "      <td>{!, hey}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(hey, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13806</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>tuffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>nope.</td>\n",
       "      <td>419</td>\n",
       "      <td>2014</td>\n",
       "      <td>[nope, .]</td>\n",
       "      <td>{nope, .}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(nope, NN), (., .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13814</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>drago!</td>\n",
       "      <td>427</td>\n",
       "      <td>2014</td>\n",
       "      <td>[drago, !]</td>\n",
       "      <td>{!, drago}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(drago, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13820</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>astrid</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>433</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{no, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13825</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>we?</td>\n",
       "      <td>438</td>\n",
       "      <td>2014</td>\n",
       "      <td>[we, ?]</td>\n",
       "      <td>{we, ?}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(we, PRP), (?, .)]</td>\n",
       "      <td>{'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13896</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, no...</td>\n",
       "      <td>509</td>\n",
       "      <td>2014</td>\n",
       "      <td>[oh, ,, no, ...]</td>\n",
       "      <td>{..., oh, no, ,}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (no, DT), (..., :)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'DT': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13904</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drago</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what?!</td>\n",
       "      <td>517</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ?, !]</td>\n",
       "      <td>{!, what, ?}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(what, WP), (?, .), (!, .)]</td>\n",
       "      <td>{'WP': 1, '.': 2}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13927</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>another one?</td>\n",
       "      <td>540</td>\n",
       "      <td>2014</td>\n",
       "      <td>[another, one, ?]</td>\n",
       "      <td>{another, one, ?}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(another, DT), (one, CD), (?, .)]</td>\n",
       "      <td>{'DT': 1, 'CD': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13938</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>f</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>valka</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>551</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{no, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13941</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>stoick</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>554</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{no, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13958</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>stoick</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what...?</td>\n",
       "      <td>571</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ..., ?]</td>\n",
       "      <td>{..., what, ?}</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[(what, WP), (..., :), (?, .)]</td>\n",
       "      <td>{'WP': 1, ':': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13975</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no... you...</td>\n",
       "      <td>588</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, ..., you, ...]</td>\n",
       "      <td>{..., you, no}</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>[(no, DT), (..., :), (you, PRP), (..., :)]</td>\n",
       "      <td>{'DT': 1, ':': 2, 'PRP': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13991</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>tuffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>uh, with what?</td>\n",
       "      <td>604</td>\n",
       "      <td>2014</td>\n",
       "      <td>[uh, ,, with, what, ?]</td>\n",
       "      <td>{?, with, ,, what, uh}</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[(uh, NN), (,, ,), (with, IN), (what, WP), (?,...</td>\n",
       "      <td>{'NN': 1, ',': 1, 'IN': 1, 'WP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14012</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no...</td>\n",
       "      <td>625</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, ...]</td>\n",
       "      <td>{..., no}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (..., :)]</td>\n",
       "      <td>{'DT': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>tuffnut</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>uh... how?</td>\n",
       "      <td>628</td>\n",
       "      <td>2014</td>\n",
       "      <td>[uh, ..., how, ?]</td>\n",
       "      <td>{..., how, uh, ?}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(uh, NN), (..., :), (how, WRB), (?, .)]</td>\n",
       "      <td>{'NN': 1, ':': 1, 'WRB': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14020</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drago</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>what?</td>\n",
       "      <td>633</td>\n",
       "      <td>2014</td>\n",
       "      <td>[what, ?]</td>\n",
       "      <td>{what, ?}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(what, WP), (?, .)]</td>\n",
       "      <td>{'WP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14024</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>HELPER</td>\n",
       "      <td>D</td>\n",
       "      <td>snotlout</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>uh-oh...</td>\n",
       "      <td>637</td>\n",
       "      <td>2014</td>\n",
       "      <td>[uh-oh, ...]</td>\n",
       "      <td>{..., uh-oh}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(uh-oh, JJ), (..., :)]</td>\n",
       "      <td>{'JJ': 1, ':': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14036</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>drago</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>no!</td>\n",
       "      <td>649</td>\n",
       "      <td>2014</td>\n",
       "      <td>[no, !]</td>\n",
       "      <td>{no, !}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(no, DT), (!, .)]</td>\n",
       "      <td>{'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14037</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>yeah!</td>\n",
       "      <td>650</td>\n",
       "      <td>2014</td>\n",
       "      <td>[yeah, !]</td>\n",
       "      <td>{!, yeah}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(yeah, NN), (!, .)]</td>\n",
       "      <td>{'NN': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>PRO</td>\n",
       "      <td>D</td>\n",
       "      <td>hiccup</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>oh, no!</td>\n",
       "      <td>672</td>\n",
       "      <td>2014</td>\n",
       "      <td>[oh, ,, no, !]</td>\n",
       "      <td>{no, !, oh, ,}</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[(oh, UH), (,, ,), (no, DT), (!, .)]</td>\n",
       "      <td>{'UH': 1, ',': 1, 'DT': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14080</th>\n",
       "      <td>DREAMWORKS</td>\n",
       "      <td>m</td>\n",
       "      <td>How to Train Your Dragon 2</td>\n",
       "      <td>ANT</td>\n",
       "      <td>D</td>\n",
       "      <td>eret</td>\n",
       "      <td>NON-P</td>\n",
       "      <td>me?</td>\n",
       "      <td>693</td>\n",
       "      <td>2014</td>\n",
       "      <td>[me, ?]</td>\n",
       "      <td>{me, ?}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(me, PRP), (?, .)]</td>\n",
       "      <td>{'PRP': 1, '.': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Disney_Period Gender                       Movie    Role Song  \\\n",
       "9             EARLY      f                  Snow White     PRO    D   \n",
       "10            EARLY      m                  Snow White     PRO    D   \n",
       "11            EARLY      f                  Snow White     PRO    D   \n",
       "25            EARLY      f                  Snow White     PRO    D   \n",
       "64            EARLY      m                  Snow White  HELPER    D   \n",
       "91            EARLY      m                  Snow White  HELPER    D   \n",
       "95            EARLY      m                  Snow White  HELPER    D   \n",
       "97            EARLY      m                  Snow White  HELPER    D   \n",
       "135           EARLY      m                  Snow White  HELPER    D   \n",
       "138           EARLY      f                  Snow White     PRO    D   \n",
       "148           EARLY      m                  Snow White  HELPER    D   \n",
       "239           EARLY      m                  Snow White  HELPER    D   \n",
       "292           EARLY      m                  Snow White  HELPER    D   \n",
       "300           EARLY      m                  Snow White  HELPER    D   \n",
       "317           EARLY      f                  Snow White     ANT    D   \n",
       "352           EARLY      f                  Snow White     PRO    D   \n",
       "354           EARLY      f                  Snow White     PRO    D   \n",
       "369           EARLY      n                  Cinderella       N    D   \n",
       "378           EARLY      n                  Cinderella  HELPER    D   \n",
       "416           EARLY      n                  Cinderella  HELPER    D   \n",
       "417           EARLY      m                  Cinderella  HELPER    D   \n",
       "510           EARLY      f                  Cinderella     ANT    D   \n",
       "522           EARLY      f                  Cinderella     ANT    D   \n",
       "565           EARLY      n                  Cinderella  HELPER    D   \n",
       "587           EARLY      f                  Cinderella     PRO    D   \n",
       "627           EARLY      n                  Cinderella       N    D   \n",
       "633           EARLY      f                  Cinderella     PRO    D   \n",
       "635           EARLY      f                  Cinderella     PRO    D   \n",
       "668           EARLY      m                  Cinderella       N    D   \n",
       "679           EARLY      m                  Cinderella     PRO    D   \n",
       "...             ...    ...                         ...     ...  ...   \n",
       "13556    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13560    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13603    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13609    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13615    DREAMWORKS      f  How to Train Your Dragon 2  HELPER    D   \n",
       "13628    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13728    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13778    DREAMWORKS      f  How to Train Your Dragon 2     PRO    D   \n",
       "13781    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13787    DREAMWORKS      f  How to Train Your Dragon 2  HELPER    D   \n",
       "13806    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13814    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13820    DREAMWORKS      f  How to Train Your Dragon 2     PRO    D   \n",
       "13825    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13896    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13904    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "13927    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13938    DREAMWORKS      f  How to Train Your Dragon 2  HELPER    D   \n",
       "13941    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13958    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "13975    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "13991    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "14012    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "14015    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "14020    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "14024    DREAMWORKS      m  How to Train Your Dragon 2  HELPER    D   \n",
       "14036    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "14037    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "14059    DREAMWORKS      m  How to Train Your Dragon 2     PRO    D   \n",
       "14080    DREAMWORKS      m  How to Train Your Dragon 2     ANT    D   \n",
       "\n",
       "              Speaker Speaker_Status                          Text  \\\n",
       "9          snow white       PRINCESS                         oh !    \n",
       "10             prince         PRINCE                       hello.    \n",
       "11         snow white       PRINCESS                          oh.    \n",
       "25         snow white       PRINCESS                 butbut who ?    \n",
       "64              happy          NON-P                        gosh.    \n",
       "91             sneezy          NON-P                   oh, ahahah    \n",
       "95              happy          NON-P  whawhat's that ? that's it.    \n",
       "97                doc          NON-P               it's up there.    \n",
       "135           bashful          NON-P                   oh, gosh !    \n",
       "138        snow white       PRINCESS                    and you ?    \n",
       "148               doc          NON-P                    oh, yeah.    \n",
       "239            sneezy          NON-P      hey ! hey ! hey ! hey !    \n",
       "292            sneezy          NON-P                   oh, gosh !    \n",
       "300            grumpy          NON-P                        huh !    \n",
       "317             queen          NON-P                    ah ! ah !    \n",
       "352        snow white       PRINCESS                          oh.    \n",
       "354        snow white       PRINCESS                          oh.    \n",
       "369    birds chirping          NON-P                         why?    \n",
       "378              mice          NON-P                          he!    \n",
       "416              mice          NON-P                          up!    \n",
       "417               gus          NON-P                          up!    \n",
       "510          drizella          NON-P              why, that's us!    \n",
       "522          drizella          NON-P                      oh \"if\"    \n",
       "565             mouse          NON-P                          oh!    \n",
       "587        cinderella       PRINCESS                       oh no!    \n",
       "627             horse          NON-P                         ahem    \n",
       "633        cinderella       PRINCESS                 another one?    \n",
       "635        cinderella       PRINCESS                       but uh    \n",
       "668        grand duke          NON-P                        ahem.    \n",
       "679            prince         PRINCE                     but why?    \n",
       "...               ...            ...                           ...   \n",
       "13556        snotlout          NON-P                       yeah...   \n",
       "13560          hiccup          NON-P                   what? why?!   \n",
       "13603            eret          NON-P                          why?   \n",
       "13609          hiccup          NON-P                        may i?   \n",
       "13615         ruffnut          NON-P                       oh, my.   \n",
       "13628          hiccup          NON-P                           no.   \n",
       "13728          hiccup          NON-P                          wow.   \n",
       "13778          astrid          NON-P            what's down there?   \n",
       "13781            eret          NON-P                        drago!   \n",
       "13787         ruffnut          NON-P                          hey!   \n",
       "13806         tuffnut          NON-P                         nope.   \n",
       "13814            eret          NON-P                        drago!   \n",
       "13820          astrid          NON-P                           no!   \n",
       "13825          hiccup          NON-P                           we?   \n",
       "13896          hiccup          NON-P                     oh, no...   \n",
       "13904           drago          NON-P                        what?!   \n",
       "13927          hiccup          NON-P                  another one?   \n",
       "13938           valka          NON-P                           no!   \n",
       "13941          stoick          NON-P                           no!   \n",
       "13958          stoick          NON-P                      what...?   \n",
       "13975          hiccup          NON-P                  no... you...   \n",
       "13991         tuffnut          NON-P                uh, with what?   \n",
       "14012          hiccup          NON-P                         no...   \n",
       "14015         tuffnut          NON-P                    uh... how?   \n",
       "14020           drago          NON-P                         what?   \n",
       "14024        snotlout          NON-P                      uh-oh...   \n",
       "14036           drago          NON-P                           no!   \n",
       "14037          hiccup          NON-P                         yeah!   \n",
       "14059          hiccup          NON-P                       oh, no!   \n",
       "14080            eret          NON-P                           me?   \n",
       "\n",
       "       UTTERANCE_NUMBER  Year                                   Tokens  \\\n",
       "9                    10  1937                                  [oh, !]   \n",
       "10                   11  1937                               [hello, .]   \n",
       "11                   12  1937                                  [oh, .]   \n",
       "25                   26  1937                         [butbut, who, ?]   \n",
       "64                   65  1937                                [gosh, .]   \n",
       "91                   92  1937                          [oh, ,, ahahah]   \n",
       "95                   96  1937  [whawhat, 's, that, ?, that, 's, it, .]   \n",
       "97                   98  1937                   [it, 's, up, there, .]   \n",
       "135                 136  1937                         [oh, ,, gosh, !]   \n",
       "138                 139  1937                            [and, you, ?]   \n",
       "148                 149  1937                         [oh, ,, yeah, .]   \n",
       "239                 240  1937         [hey, !, hey, !, hey, !, hey, !]   \n",
       "292                 293  1937                         [oh, ,, gosh, !]   \n",
       "300                 301  1937                                 [huh, !]   \n",
       "317                 318  1937                           [ah, !, ah, !]   \n",
       "352                 353  1937                                  [oh, .]   \n",
       "354                 355  1937                                  [oh, .]   \n",
       "369                   7  1950                                 [why, ?]   \n",
       "378                  16  1950                                  [he, !]   \n",
       "416                  54  1950                                  [up, !]   \n",
       "417                  55  1950                                  [up, !]   \n",
       "510                 148  1950                [why, ,, that, 's, us, !]   \n",
       "522                 160  1950                         [oh, ``, if, '']   \n",
       "565                 203  1950                                  [oh, !]   \n",
       "587                 225  1950                              [oh, no, !]   \n",
       "627                 265  1950                                   [ahem]   \n",
       "633                 271  1950                        [another, one, ?]   \n",
       "635                 273  1950                                [but, uh]   \n",
       "668                 306  1950                                [ahem, .]   \n",
       "679                 317  1950                            [but, why, ?]   \n",
       "...                 ...   ...                                      ...   \n",
       "13556               169  2014                              [yeah, ...]   \n",
       "13560               173  2014                     [what, ?, why, ?, !]   \n",
       "13603               216  2014                                 [why, ?]   \n",
       "13609               222  2014                              [may, i, ?]   \n",
       "13615               228  2014                           [oh, ,, my, .]   \n",
       "13628               241  2014                                  [no, .]   \n",
       "13728               341  2014                                 [wow, .]   \n",
       "13778               391  2014               [what, 's, down, there, ?]   \n",
       "13781               394  2014                               [drago, !]   \n",
       "13787               400  2014                                 [hey, !]   \n",
       "13806               419  2014                                [nope, .]   \n",
       "13814               427  2014                               [drago, !]   \n",
       "13820               433  2014                                  [no, !]   \n",
       "13825               438  2014                                  [we, ?]   \n",
       "13896               509  2014                         [oh, ,, no, ...]   \n",
       "13904               517  2014                             [what, ?, !]   \n",
       "13927               540  2014                        [another, one, ?]   \n",
       "13938               551  2014                                  [no, !]   \n",
       "13941               554  2014                                  [no, !]   \n",
       "13958               571  2014                           [what, ..., ?]   \n",
       "13975               588  2014                      [no, ..., you, ...]   \n",
       "13991               604  2014                   [uh, ,, with, what, ?]   \n",
       "14012               625  2014                                [no, ...]   \n",
       "14015               628  2014                        [uh, ..., how, ?]   \n",
       "14020               633  2014                                [what, ?]   \n",
       "14024               637  2014                             [uh-oh, ...]   \n",
       "14036               649  2014                                  [no, !]   \n",
       "14037               650  2014                                [yeah, !]   \n",
       "14059               672  2014                           [oh, ,, no, !]   \n",
       "14080               693  2014                                  [me, ?]   \n",
       "\n",
       "                               Types  Token_Count  Type_Count  \\\n",
       "9                            {!, oh}            2           2   \n",
       "10                        {., hello}            2           2   \n",
       "11                           {oh, .}            2           2   \n",
       "25                  {butbut, ?, who}            3           3   \n",
       "64                         {gosh, .}            2           2   \n",
       "91                   {oh, ahahah, ,}            3           3   \n",
       "95     {whawhat, 's, ?, ., that, it}            8           6   \n",
       "97            {'s, ., there, up, it}            5           5   \n",
       "135                 {gosh, !, oh, ,}            4           4   \n",
       "138                    {you, ?, and}            3           3   \n",
       "148                 {oh, ., yeah, ,}            4           4   \n",
       "239                         {!, hey}            8           2   \n",
       "292                 {gosh, !, oh, ,}            4           4   \n",
       "300                         {!, huh}            2           2   \n",
       "317                          {!, ah}            4           2   \n",
       "352                          {oh, .}            2           2   \n",
       "354                          {oh, .}            2           2   \n",
       "369                         {why, ?}            2           2   \n",
       "378                          {he, !}            2           2   \n",
       "416                          {up, !}            2           2   \n",
       "417                          {up, !}            2           2   \n",
       "510        {!, 's, us, that, why, ,}            6           6   \n",
       "522                 {if, oh, '', ``}            4           4   \n",
       "565                          {!, oh}            2           2   \n",
       "587                      {no, !, oh}            3           3   \n",
       "627                           {ahem}            1           1   \n",
       "633                {another, one, ?}            3           3   \n",
       "635                        {but, uh}            2           2   \n",
       "668                        {ahem, .}            2           2   \n",
       "679                    {why, but, ?}            3           3   \n",
       "...                              ...          ...         ...   \n",
       "13556                    {..., yeah}            2           2   \n",
       "13560              {why, !, what, ?}            5           4   \n",
       "13603                       {why, ?}            2           2   \n",
       "13609                    {i, ?, may}            3           3   \n",
       "13615                 {my, oh, ., ,}            4           4   \n",
       "13628                        {no, .}            2           2   \n",
       "13728                       {., wow}            2           2   \n",
       "13778     {'s, ?, down, there, what}            5           5   \n",
       "13781                     {!, drago}            2           2   \n",
       "13787                       {!, hey}            2           2   \n",
       "13806                      {nope, .}            2           2   \n",
       "13814                     {!, drago}            2           2   \n",
       "13820                        {no, !}            2           2   \n",
       "13825                        {we, ?}            2           2   \n",
       "13896               {..., oh, no, ,}            4           4   \n",
       "13904                   {!, what, ?}            3           3   \n",
       "13927              {another, one, ?}            3           3   \n",
       "13938                        {no, !}            2           2   \n",
       "13941                        {no, !}            2           2   \n",
       "13958                 {..., what, ?}            3           3   \n",
       "13975                 {..., you, no}            4           3   \n",
       "13991         {?, with, ,, what, uh}            5           5   \n",
       "14012                      {..., no}            2           2   \n",
       "14015              {..., how, uh, ?}            4           4   \n",
       "14020                      {what, ?}            2           2   \n",
       "14024                   {..., uh-oh}            2           2   \n",
       "14036                        {no, !}            2           2   \n",
       "14037                      {!, yeah}            2           2   \n",
       "14059                 {no, !, oh, ,}            4           4   \n",
       "14080                        {me, ?}            2           2   \n",
       "\n",
       "                                                     POS  \\\n",
       "9                                     [(oh, UH), (!, .)]   \n",
       "10                                 [(hello, NN), (., .)]   \n",
       "11                                    [(oh, UH), (., .)]   \n",
       "25                     [(butbut, NN), (who, WP), (?, .)]   \n",
       "64                                  [(gosh, NN), (., .)]   \n",
       "91                      [(oh, UH), (,, ,), (ahahah, JJ)]   \n",
       "95     [(whawhat, WP), ('s, VBZ), (that, DT), (?, .),...   \n",
       "97     [(it, PRP), ('s, VBZ), (up, RP), (there, RB), ...   \n",
       "135               [(oh, UH), (,, ,), (gosh, JJ), (!, .)]   \n",
       "138                      [(and, CC), (you, PRP), (?, .)]   \n",
       "148               [(oh, UH), (,, ,), (yeah, UH), (., .)]   \n",
       "239    [(hey, NN), (!, .), (hey, NN), (!, .), (hey, N...   \n",
       "292               [(oh, UH), (,, ,), (gosh, JJ), (!, .)]   \n",
       "300                                  [(huh, NN), (!, .)]   \n",
       "317                 [(ah, NN), (!, .), (ah, NN), (!, .)]   \n",
       "352                                   [(oh, UH), (., .)]   \n",
       "354                                   [(oh, UH), (., .)]   \n",
       "369                                 [(why, WRB), (?, .)]   \n",
       "378                                  [(he, PRP), (!, .)]   \n",
       "416                                   [(up, RB), (!, .)]   \n",
       "417                                   [(up, RB), (!, .)]   \n",
       "510    [(why, WRB), (,, ,), (that, DT), ('s, VBZ), (u...   \n",
       "522             [(oh, UH), (``, ``), (if, IN), ('', '')]   \n",
       "565                                   [(oh, UH), (!, .)]   \n",
       "587                         [(oh, UH), (no, DT), (!, .)]   \n",
       "627                                         [(ahem, NN)]   \n",
       "633                   [(another, DT), (one, CD), (?, .)]   \n",
       "635                                [(but, CC), (uh, NN)]   \n",
       "668                                 [(ahem, NN), (., .)]   \n",
       "679                      [(but, CC), (why, WRB), (?, .)]   \n",
       "...                                                  ...   \n",
       "13556                             [(yeah, NN), (..., :)]   \n",
       "13560   [(what, WP), (?, .), (why, WRB), (?, .), (!, .)]   \n",
       "13603                               [(why, WRB), (?, .)]   \n",
       "13609                       [(may, MD), (i, VB), (?, .)]   \n",
       "13615             [(oh, UH), (,, ,), (my, PRP$), (., .)]   \n",
       "13628                                 [(no, DT), (., .)]   \n",
       "13728                                [(wow, NN), (., .)]   \n",
       "13778  [(what, WP), ('s, VBZ), (down, RB), (there, RB...   \n",
       "13781                              [(drago, NN), (!, .)]   \n",
       "13787                                [(hey, NN), (!, .)]   \n",
       "13806                               [(nope, NN), (., .)]   \n",
       "13814                              [(drago, NN), (!, .)]   \n",
       "13820                                 [(no, DT), (!, .)]   \n",
       "13825                                [(we, PRP), (?, .)]   \n",
       "13896             [(oh, UH), (,, ,), (no, DT), (..., :)]   \n",
       "13904                       [(what, WP), (?, .), (!, .)]   \n",
       "13927                 [(another, DT), (one, CD), (?, .)]   \n",
       "13938                                 [(no, DT), (!, .)]   \n",
       "13941                                 [(no, DT), (!, .)]   \n",
       "13958                     [(what, WP), (..., :), (?, .)]   \n",
       "13975         [(no, DT), (..., :), (you, PRP), (..., :)]   \n",
       "13991  [(uh, NN), (,, ,), (with, IN), (what, WP), (?,...   \n",
       "14012                               [(no, DT), (..., :)]   \n",
       "14015           [(uh, NN), (..., :), (how, WRB), (?, .)]   \n",
       "14020                               [(what, WP), (?, .)]   \n",
       "14024                            [(uh-oh, JJ), (..., :)]   \n",
       "14036                                 [(no, DT), (!, .)]   \n",
       "14037                               [(yeah, NN), (!, .)]   \n",
       "14059               [(oh, UH), (,, ,), (no, DT), (!, .)]   \n",
       "14080                                [(me, PRP), (?, .)]   \n",
       "\n",
       "                                                Tag_Freq  Command_Count  \\\n",
       "9                                      {'UH': 1, '.': 1}              0   \n",
       "10                                     {'NN': 1, '.': 1}              0   \n",
       "11                                     {'UH': 1, '.': 1}              0   \n",
       "25                            {'NN': 1, 'WP': 1, '.': 1}              0   \n",
       "64                                     {'NN': 1, '.': 1}              0   \n",
       "91                            {'UH': 1, ',': 1, 'JJ': 1}              0   \n",
       "95        {'WP': 1, 'VBZ': 2, 'DT': 2, '.': 2, 'PRP': 1}              0   \n",
       "97        {'PRP': 1, 'VBZ': 1, 'RP': 1, 'RB': 1, '.': 1}              0   \n",
       "135                   {'UH': 1, ',': 1, 'JJ': 1, '.': 1}              0   \n",
       "138                          {'CC': 1, 'PRP': 1, '.': 1}              0   \n",
       "148                            {'UH': 2, ',': 1, '.': 1}              0   \n",
       "239                                    {'NN': 4, '.': 4}              0   \n",
       "292                   {'UH': 1, ',': 1, 'JJ': 1, '.': 1}              0   \n",
       "300                                    {'NN': 1, '.': 1}              0   \n",
       "317                                    {'NN': 2, '.': 2}              0   \n",
       "352                                    {'UH': 1, '.': 1}              0   \n",
       "354                                    {'UH': 1, '.': 1}              0   \n",
       "369                                   {'WRB': 1, '.': 1}              0   \n",
       "378                                   {'PRP': 1, '.': 1}              0   \n",
       "416                                    {'RB': 1, '.': 1}              0   \n",
       "417                                    {'RB': 1, '.': 1}              0   \n",
       "510    {'WRB': 1, ',': 1, 'DT': 1, 'VBZ': 1, 'PRP': 1...              0   \n",
       "522                 {'UH': 1, '``': 1, 'IN': 1, '''': 1}              0   \n",
       "565                                    {'UH': 1, '.': 1}              0   \n",
       "587                           {'UH': 1, 'DT': 1, '.': 1}              0   \n",
       "627                                            {'NN': 1}              0   \n",
       "633                           {'DT': 1, 'CD': 1, '.': 1}              0   \n",
       "635                                   {'CC': 1, 'NN': 1}              0   \n",
       "668                                    {'NN': 1, '.': 1}              0   \n",
       "679                          {'CC': 1, 'WRB': 1, '.': 1}              0   \n",
       "...                                                  ...            ...   \n",
       "13556                                  {'NN': 1, ':': 1}              0   \n",
       "13560                        {'WP': 1, '.': 3, 'WRB': 1}              0   \n",
       "13603                                 {'WRB': 1, '.': 1}              0   \n",
       "13609                         {'MD': 1, 'VB': 1, '.': 1}              1   \n",
       "13615               {'UH': 1, ',': 1, 'PRP$': 1, '.': 1}              0   \n",
       "13628                                  {'DT': 1, '.': 1}              0   \n",
       "13728                                  {'NN': 1, '.': 1}              0   \n",
       "13778               {'WP': 1, 'VBZ': 1, 'RB': 2, '.': 1}              0   \n",
       "13781                                  {'NN': 1, '.': 1}              0   \n",
       "13787                                  {'NN': 1, '.': 1}              0   \n",
       "13806                                  {'NN': 1, '.': 1}              0   \n",
       "13814                                  {'NN': 1, '.': 1}              0   \n",
       "13820                                  {'DT': 1, '.': 1}              0   \n",
       "13825                                 {'PRP': 1, '.': 1}              0   \n",
       "13896                 {'UH': 1, ',': 1, 'DT': 1, ':': 1}              0   \n",
       "13904                                  {'WP': 1, '.': 2}              0   \n",
       "13927                         {'DT': 1, 'CD': 1, '.': 1}              0   \n",
       "13938                                  {'DT': 1, '.': 1}              0   \n",
       "13941                                  {'DT': 1, '.': 1}              0   \n",
       "13958                          {'WP': 1, ':': 1, '.': 1}              0   \n",
       "13975                        {'DT': 1, ':': 2, 'PRP': 1}              0   \n",
       "13991        {'NN': 1, ',': 1, 'IN': 1, 'WP': 1, '.': 1}              0   \n",
       "14012                                  {'DT': 1, ':': 1}              0   \n",
       "14015                {'NN': 1, ':': 1, 'WRB': 1, '.': 1}              0   \n",
       "14020                                  {'WP': 1, '.': 1}              0   \n",
       "14024                                  {'JJ': 1, ':': 1}              0   \n",
       "14036                                  {'DT': 1, '.': 1}              0   \n",
       "14037                                  {'NN': 1, '.': 1}              0   \n",
       "14059                 {'UH': 1, ',': 1, 'DT': 1, '.': 1}              0   \n",
       "14080                                 {'PRP': 1, '.': 1}              0   \n",
       "\n",
       "      Lemmas  Len_Lemmas  \n",
       "9         []           0  \n",
       "10        []           0  \n",
       "11        []           0  \n",
       "25        []           0  \n",
       "64        []           0  \n",
       "91        []           0  \n",
       "95        []           0  \n",
       "97        []           0  \n",
       "135       []           0  \n",
       "138       []           0  \n",
       "148       []           0  \n",
       "239       []           0  \n",
       "292       []           0  \n",
       "300       []           0  \n",
       "317       []           0  \n",
       "352       []           0  \n",
       "354       []           0  \n",
       "369       []           0  \n",
       "378       []           0  \n",
       "416       []           0  \n",
       "417       []           0  \n",
       "510       []           0  \n",
       "522       []           0  \n",
       "565       []           0  \n",
       "587       []           0  \n",
       "627       []           0  \n",
       "633       []           0  \n",
       "635       []           0  \n",
       "668       []           0  \n",
       "679       []           0  \n",
       "...      ...         ...  \n",
       "13556     []           0  \n",
       "13560     []           0  \n",
       "13603     []           0  \n",
       "13609     []           0  \n",
       "13615     []           0  \n",
       "13628     []           0  \n",
       "13728     []           0  \n",
       "13778     []           0  \n",
       "13781     []           0  \n",
       "13787     []           0  \n",
       "13806     []           0  \n",
       "13814     []           0  \n",
       "13820     []           0  \n",
       "13825     []           0  \n",
       "13896     []           0  \n",
       "13904     []           0  \n",
       "13927     []           0  \n",
       "13938     []           0  \n",
       "13941     []           0  \n",
       "13958     []           0  \n",
       "13975     []           0  \n",
       "13991     []           0  \n",
       "14012     []           0  \n",
       "14015     []           0  \n",
       "14020     []           0  \n",
       "14024     []           0  \n",
       "14036     []           0  \n",
       "14037     []           0  \n",
       "14059     []           0  \n",
       "14080     []           0  \n",
       "\n",
       "[718 rows x 19 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df[movies_df.Len_Lemmas < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " #yeeshh, 718 not included???\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Text_No_Punc'] = movies_df.Text.map(lambda x: re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '',x))\n",
    "#this gets rid of apostrophes too, which may be problematic...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slave in the magic mirror come from the farthest space through wind and darkness i summon thee speak  let me see thy face '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df['Text_No_Punc'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Lemmas'] = movies_df.Text_No_Punc.map(lambda x: [wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slave', 'magic', 'mirror', 'come', 'farthest', 'space', 'wind', 'darkness', 'summon', 'let', 'see', 'thy', 'face'] \n",
      "\n",
      "['wouldst', 'know', 'queen'] \n",
      "\n",
      "['magic', 'mirror', 'wall', 'be', 'fairest'] \n",
      "\n",
      "['fame', 'be', 'thy', 'beauty', 'majesty', 'hold', 'lovely', 'maid', 'see', 'rag', 'hide', 'gentle', 'grace', 'be', 'more', 'fair'] \n",
      "\n",
      "['reveal', 'name'] \n",
      "\n",
      "['lip', 'red', 'rise', 'hair', 'black', 'ebony', 'skin', 'white', 'snow'] \n",
      "\n",
      "['snow', 'white'] \n",
      "\n",
      "['today'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 11):\n",
    "    print(movies_df.Lemmas.iloc[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     slave in the magic mirror come from the farthe...\n",
       "1                     what wouldst thou know my queen  \n",
       "2     magic mirror on the wall who is the fairest on...\n",
       "3     famed is thy beauty majesty but hold a lovely ...\n",
       "4                        alas for her  reveal her name \n",
       "5     lips red as the rose hair black as ebony skin ...\n",
       "6                                          snow white  \n",
       "8                                                today \n",
       "9                                                  oh  \n",
       "10                                               hello \n",
       "11                                                  oh \n",
       "Name: Text_No_Punc, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Text_No_Punc[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's try another method....\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cassi\\Desktop\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-4202e797c4d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmovies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m         \u001b[0mforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mapply_rules\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m             return [form[:-len(old)] + new\n\u001b[1;32m-> 1823\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m                     if form.endswith(old)]\n",
      "\u001b[1;32mc:\\users\\cassi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1823\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1824\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1825\u001b[1;33m                     if form.endswith(old)]\n\u001b[0m\u001b[0;32m   1826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1827\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "[lemmatizer.lemmatize((w, get_wordnet_pos(w)) for w in movies_df.Tokens.iloc[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features = 500, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Lemmas'] = movies_df.Lemmas.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slave magic mirror come farthest space wind da...\n",
       "1                                   wouldst know queen\n",
       "2                         magic mirror wall be fairest\n",
       "3    fame be thy beauty majesty hold lovely maid se...\n",
       "4                                          reveal name\n",
       "Name: Lemmas, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.Lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_tfidf = tfidf_vectorizer.fit_transform(movies_df['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_topics(model, tfidf_vectorizer, n_top_words):\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Picking number of topics and number of words\n",
    "number_topics = 10\n",
    "number_words = 5\n",
    "\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "man guy shrek hiccup maybe\n",
      "\n",
      "Topic #1:\n",
      "look say let tell thing\n",
      "\n",
      "Topic #2:\n",
      "come know okay just wait\n",
      "\n",
      "Topic #3:\n",
      "think really dragon make listen\n",
      "\n",
      "Topic #4:\n",
      "princess dad dear mom home\n",
      "\n",
      "Topic #5:\n",
      "right talk hold stay ready\n",
      "\n",
      "Topic #6:\n",
      "yes sir need isn ll\n",
      "\n",
      "Topic #7:\n",
      "thank sorry don mean love\n",
      "\n",
      "Topic #8:\n",
      "stop time sure bring place\n",
      "\n",
      "Topic #9:\n",
      "way help boy girl want\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "man guy shrek hiccup maybe head master life watch rapunzel\n",
      "\n",
      "Topic #1:\n",
      "look say let tell thing prince ve work bad gonna\n",
      "\n",
      "Topic #2:\n",
      "come know okay just wait don didn try course better\n",
      "\n",
      "Topic #3:\n",
      "think really dragon make listen win new kill believe jack\n",
      "\n",
      "Topic #4:\n",
      "princess dad dear mom home eat queen fight big live\n",
      "\n",
      "Topic #5:\n",
      "right talk hold stay ready run hear turn majesty hmm\n",
      "\n",
      "Topic #6:\n",
      "yes sir need isn ll tooth like understand miss ya\n",
      "\n",
      "Topic #7:\n",
      "thank sorry don mean love happen night away gotta beautiful\n",
      "\n",
      "Topic #8:\n",
      "stop time sure bring place fine great hand die ariel\n",
      "\n",
      "Topic #9:\n",
      "way help boy girl want whoa wrong little kid dream\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_movies = movies_df[movies_df.Gender == 'f']\n",
    "m_movies = movies_df[movies_df.Gender == 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_movies_tfidf = tfidf_vectorizer.fit_transform(f_movies['Lemmas'])\n",
    "m_movies_tfidf = tfidf_vectorizer.fit_transform(m_movies['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(f_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "rule deep mama whoa dangerous\n",
      "\n",
      "Topic #1:\n",
      "light long ll throw book\n",
      "\n",
      "Topic #2:\n",
      "ogre street sight dark little\n",
      "\n",
      "Topic #3:\n",
      "sorry question gonna half great\n",
      "\n",
      "Topic #4:\n",
      "close sea think lucky red\n",
      "\n",
      "Topic #5:\n",
      "year peace war tell child\n",
      "\n",
      "Topic #6:\n",
      "thunk thought way wait metro\n",
      "\n",
      "Topic #7:\n",
      "doesn land look use wrong\n",
      "\n",
      "Topic #8:\n",
      "nice tonight cool live mess\n",
      "\n",
      "Topic #9:\n",
      "kind hold return quick let\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(m_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "way good wait mean isn\n",
      "\n",
      "Topic #1:\n",
      "right don time need head\n",
      "\n",
      "Topic #2:\n",
      "look stop dragon just try\n",
      "\n",
      "Topic #3:\n",
      "know let sorry believe work\n",
      "\n",
      "Topic #4:\n",
      "come yes say help didn\n",
      "\n",
      "Topic #5:\n",
      "really princess thank tell hold\n",
      "\n",
      "Topic #6:\n",
      "okay ll ya away sure\n",
      "\n",
      "Topic #7:\n",
      "whoa happen gotta toothless feel\n",
      "\n",
      "Topic #8:\n",
      "boy think guy talk shrek\n",
      "\n",
      "Topic #9:\n",
      "man dad love gonna thing\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_topics(model, tfidf_vectorizer, n_top_words):\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Picking number of topics and number of words\n",
    "number_topics = 5\n",
    "number_words = 5\n",
    "\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "little gold read water power\n",
      "\n",
      "Topic #1:\n",
      "king dinner tell lucky thank\n",
      "\n",
      "Topic #2:\n",
      "rest long step somebody village\n",
      "\n",
      "Topic #3:\n",
      "think job love goin like\n",
      "\n",
      "Topic #4:\n",
      "close learn yes noodle run\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use the raw text...\n",
    "movies_tfidf = tfidf_vectorizer.fit_transform(movies_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_topics = 5\n",
    "number_words = 5\n",
    "\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "know okay think want stop\n",
      "\n",
      "Topic #1:\n",
      "yeah right hey going really\n",
      "\n",
      "Topic #2:\n",
      "oh come yes did just\n",
      "\n",
      "Topic #3:\n",
      "let uh like ll time\n",
      "\n",
      "Topic #4:\n",
      "look doing help got hiccup\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_movies_tfidf = tfidf_vectorizer.fit_transform(f_movies['Text'])\n",
    "m_movies_tfidf = tfidf_vectorizer.fit_transform(m_movies['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(f_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "years kingdom pick mad wanted\n",
      "\n",
      "Topic #1:\n",
      "phillip course doesn late guardian\n",
      "\n",
      "Topic #2:\n",
      "wake hah rat thing rider\n",
      "\n",
      "Topic #3:\n",
      "nice fly ve sight look\n",
      "\n",
      "Topic #4:\n",
      "looks sandy listen tiana lose\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(m_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "look okay don got ve\n",
      "\n",
      "Topic #1:\n",
      "know just sorry want shrek\n",
      "\n",
      "Topic #2:\n",
      "oh yeah let like mean\n",
      "\n",
      "Topic #3:\n",
      "yes uh did really way\n",
      "\n",
      "Topic #4:\n",
      "come hey good ah ll\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Across roles?\n",
    "pro_movies = movies_df[movies_df.Role == 'PRO']\n",
    "ant_movies = movies_df[movies_df.Role == 'ANT']\n",
    "help_movies = movies_df[movies_df.Role == 'HELPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_movies_tfidf = tfidf_vectorizer.fit_transform(pro_movies['Lemmas'])\n",
    "ant_movies_tfidf = tfidf_vectorizer.fit_transform(ant_movies['Lemmas'])\n",
    "help_movies_tfidf = tfidf_vectorizer.fit_transform(help_movies['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(pro_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "leave zukzuk ride wait used\n",
      "\n",
      "Topic #1:\n",
      "peace save little crazy dear\n",
      "\n",
      "Topic #2:\n",
      "didn thorn matter leave maleficent\n",
      "\n",
      "Topic #3:\n",
      "lucky thing step shut stuff\n",
      "\n",
      "Topic #4:\n",
      "control kingdom tight hiccup love\n"
     ]
    }
   ],
   "source": [
    "print(\"PRO Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(ant_movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANT Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "zukzuk lucky monkey nose walk\n",
      "\n",
      "Topic #1:\n",
      "light child blue room wash\n",
      "\n",
      "Topic #2:\n",
      "kingdom egg mouth happy kiss\n",
      "\n",
      "Topic #3:\n",
      "shadow long quiet strange majesty\n",
      "\n",
      "Topic #4:\n",
      "lady clear die tia lot\n"
     ]
    }
   ],
   "source": [
    "print(\"ANT Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Attempt 2\n",
    "Maybe lines being so short might influence these topics? What if we try going by an entire pool of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_char_df = pd.read_pickle(r'C:/Users/cassi/Desktop/Data_Science/Animated-Movie-Gendered-Dialogue/private/char_toks.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 632 entries, 0 to 631\n",
      "Data columns (total 11 columns):\n",
      "Disney_Period       632 non-null object\n",
      "Gender              632 non-null object\n",
      "Movie               632 non-null object\n",
      "Role                632 non-null object\n",
      "Speaker             632 non-null object\n",
      "Speaker_Status      632 non-null object\n",
      "Total_Tok_Count     632 non-null float64\n",
      "Total_Toks          632 non-null object\n",
      "Total_Type_Count    632 non-null float64\n",
      "Total_Types         632 non-null object\n",
      "Year                632 non-null object\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 32.1+ KB\n"
     ]
    }
   ],
   "source": [
    "movie_char_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slave',\n",
       " 'in',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'mirror',\n",
       " 'come',\n",
       " 'from',\n",
       " 'the',\n",
       " 'farthest',\n",
       " 'space',\n",
       " 'through',\n",
       " 'wind',\n",
       " 'and',\n",
       " 'darkness',\n",
       " 'i',\n",
       " 'summon',\n",
       " 'thee',\n",
       " '.',\n",
       " 'speak',\n",
       " '!',\n",
       " 'let',\n",
       " 'me',\n",
       " 'see',\n",
       " 'thy',\n",
       " 'face',\n",
       " '.',\n",
       " 'magic',\n",
       " 'mirror',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wall',\n",
       " ',',\n",
       " 'who',\n",
       " 'is',\n",
       " 'the',\n",
       " 'fairest',\n",
       " 'one',\n",
       " 'of',\n",
       " 'all',\n",
       " '?',\n",
       " 'alas',\n",
       " 'for',\n",
       " 'her',\n",
       " '!',\n",
       " 'reveal',\n",
       " 'her',\n",
       " 'name',\n",
       " '.',\n",
       " 'snow',\n",
       " 'white',\n",
       " '!',\n",
       " 'take',\n",
       " 'her',\n",
       " 'far',\n",
       " 'into',\n",
       " 'the',\n",
       " 'forest',\n",
       " '.',\n",
       " 'find',\n",
       " 'some',\n",
       " 'secluded',\n",
       " 'glade',\n",
       " 'where',\n",
       " 'she',\n",
       " 'can',\n",
       " 'pick',\n",
       " 'wildflowers',\n",
       " '.',\n",
       " 'and',\n",
       " 'there',\n",
       " ',',\n",
       " 'my',\n",
       " 'faithful',\n",
       " 'huntsman',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'kill',\n",
       " 'her',\n",
       " '!',\n",
       " 'silence',\n",
       " '!',\n",
       " 'you',\n",
       " 'know',\n",
       " 'the',\n",
       " 'penalty',\n",
       " 'if',\n",
       " 'you',\n",
       " 'fail',\n",
       " '.',\n",
       " 'but',\n",
       " 'to',\n",
       " 'make',\n",
       " 'doubly',\n",
       " 'sure',\n",
       " '...',\n",
       " 'you',\n",
       " 'do',\n",
       " 'not',\n",
       " 'fail',\n",
       " ',',\n",
       " 'bring',\n",
       " 'back',\n",
       " 'her',\n",
       " 'heart',\n",
       " '...',\n",
       " 'magic',\n",
       " 'mirror',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wall',\n",
       " ',',\n",
       " 'who',\n",
       " 'now',\n",
       " 'is',\n",
       " 'the',\n",
       " 'fairest',\n",
       " 'one',\n",
       " 'of',\n",
       " 'all',\n",
       " '?',\n",
       " 'over',\n",
       " 'the',\n",
       " 'seven',\n",
       " 'jewelled',\n",
       " 'hills',\n",
       " ',',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'seventh',\n",
       " 'fall',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'cottage',\n",
       " 'of',\n",
       " 'the',\n",
       " 'seven',\n",
       " 'dwarfs',\n",
       " ',',\n",
       " 'dwells',\n",
       " 'snow',\n",
       " 'white',\n",
       " ',',\n",
       " 'fairest',\n",
       " 'one',\n",
       " 'of',\n",
       " 'all',\n",
       " '.',\n",
       " 'snow',\n",
       " 'white',\n",
       " 'lies',\n",
       " 'dead',\n",
       " 'in',\n",
       " 'the',\n",
       " 'forest',\n",
       " '.',\n",
       " 'the',\n",
       " 'huntsman',\n",
       " 'has',\n",
       " 'brought',\n",
       " 'me',\n",
       " 'proof',\n",
       " '.',\n",
       " 'behold',\n",
       " ',',\n",
       " 'her',\n",
       " 'heart',\n",
       " '.',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'of',\n",
       " 'a',\n",
       " 'pig',\n",
       " '!',\n",
       " 'then',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'tricked',\n",
       " '!',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'of',\n",
       " 'a',\n",
       " 'pig',\n",
       " '!',\n",
       " 'the',\n",
       " 'blundering',\n",
       " 'fool',\n",
       " '!',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'go',\n",
       " 'myself',\n",
       " 'to',\n",
       " 'the',\n",
       " 'dwarfs',\n",
       " \"'\",\n",
       " 'cottage',\n",
       " '...',\n",
       " 'in',\n",
       " 'a',\n",
       " 'disguise',\n",
       " 'so',\n",
       " 'complete',\n",
       " 'no',\n",
       " 'one',\n",
       " 'will',\n",
       " 'ever',\n",
       " 'suspect',\n",
       " '.',\n",
       " 'now',\n",
       " ',',\n",
       " 'a',\n",
       " 'formula',\n",
       " 'to',\n",
       " 'transform',\n",
       " 'my',\n",
       " 'beauty',\n",
       " 'into',\n",
       " 'ugliness',\n",
       " '.',\n",
       " 'change',\n",
       " 'my',\n",
       " 'queenly',\n",
       " 'raiment',\n",
       " 'to',\n",
       " 'a',\n",
       " 'peddler',\n",
       " \"'s\",\n",
       " 'cloak',\n",
       " '.',\n",
       " 'mummy',\n",
       " 'dustto',\n",
       " 'make',\n",
       " 'me',\n",
       " 'old',\n",
       " '.',\n",
       " 'to',\n",
       " 'shroud',\n",
       " 'my',\n",
       " 'clothes',\n",
       " ',',\n",
       " 'the',\n",
       " 'black',\n",
       " 'of',\n",
       " 'night',\n",
       " '.',\n",
       " 'to',\n",
       " 'age',\n",
       " 'my',\n",
       " 'voice',\n",
       " ',',\n",
       " 'an',\n",
       " 'old',\n",
       " 'hag',\n",
       " \"'s\",\n",
       " 'cackle',\n",
       " '.',\n",
       " 'to',\n",
       " 'whiten',\n",
       " 'my',\n",
       " 'hair',\n",
       " ',',\n",
       " 'a',\n",
       " 'scream',\n",
       " 'of',\n",
       " 'fright',\n",
       " '.',\n",
       " 'a',\n",
       " 'blast',\n",
       " 'of',\n",
       " 'wind',\n",
       " '...',\n",
       " 'to',\n",
       " 'fan',\n",
       " 'my',\n",
       " 'hate',\n",
       " '!',\n",
       " 'a',\n",
       " 'thunderbolt',\n",
       " '...',\n",
       " 'to',\n",
       " 'mix',\n",
       " 'it',\n",
       " 'well',\n",
       " '.',\n",
       " 'now',\n",
       " '...',\n",
       " 'begin',\n",
       " 'thy',\n",
       " 'magic',\n",
       " 'spell',\n",
       " '.',\n",
       " 'look',\n",
       " '!',\n",
       " 'my',\n",
       " 'hands',\n",
       " '!',\n",
       " 'my',\n",
       " 'voice',\n",
       " '!',\n",
       " 'my',\n",
       " 'voice',\n",
       " '!',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'disguise',\n",
       " '.',\n",
       " 'and',\n",
       " 'now',\n",
       " ',',\n",
       " 'a',\n",
       " 'special',\n",
       " '...',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'death',\n",
       " 'for',\n",
       " 'one',\n",
       " 'so',\n",
       " 'fair',\n",
       " '.',\n",
       " 'what',\n",
       " 'shall',\n",
       " 'it',\n",
       " 'be',\n",
       " '?',\n",
       " 'ah',\n",
       " '!',\n",
       " 'a',\n",
       " 'poisoned',\n",
       " 'apple',\n",
       " '!',\n",
       " 'sleeping',\n",
       " 'death',\n",
       " '.',\n",
       " 'one',\n",
       " 'taste',\n",
       " 'of',\n",
       " 'the',\n",
       " 'poisoned',\n",
       " 'apple',\n",
       " '...',\n",
       " 'and',\n",
       " 'the',\n",
       " 'victim',\n",
       " \"'s\",\n",
       " 'eyes',\n",
       " 'will',\n",
       " 'close',\n",
       " 'forever',\n",
       " '...',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sleeping',\n",
       " 'death',\n",
       " '.',\n",
       " 'yodel',\n",
       " 'holalaeeay',\n",
       " 'holalaeeay',\n",
       " 'holalaeeayeela',\n",
       " 'eeayeeleeay',\n",
       " 'holalaeeay',\n",
       " 'holalaeeay',\n",
       " 'holalaeeayeela',\n",
       " 'leeayleeoleeay',\n",
       " 'i',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'to',\n",
       " 'dance',\n",
       " 'and',\n",
       " 'tap',\n",
       " 'my',\n",
       " 'feet',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'keep',\n",
       " 'in',\n",
       " 'rhythm',\n",
       " 'you',\n",
       " 'see',\n",
       " ',',\n",
       " 'i',\n",
       " 'washed',\n",
       " \"'em\",\n",
       " 'both',\n",
       " 'today',\n",
       " 'and',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'do',\n",
       " 'nothin',\n",
       " \"'\",\n",
       " 'with',\n",
       " \"'em\",\n",
       " 'hohum',\n",
       " ',',\n",
       " 'the',\n",
       " 'tune',\n",
       " 'is',\n",
       " 'dumb',\n",
       " 'the',\n",
       " 'words',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mean',\n",
       " 'a',\n",
       " 'thing',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'this',\n",
       " 'a',\n",
       " 'silly',\n",
       " 'song',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'to',\n",
       " 'sing',\n",
       " 'i',\n",
       " 'oh',\n",
       " ',',\n",
       " 'ggosh',\n",
       " '!',\n",
       " 'i',\n",
       " 'chased',\n",
       " 'a',\n",
       " 'polecat',\n",
       " 'up',\n",
       " 'a',\n",
       " 'tree',\n",
       " 'way',\n",
       " 'out',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'limb',\n",
       " 'and',\n",
       " 'when',\n",
       " 'he',\n",
       " 'got',\n",
       " 'the',\n",
       " 'best',\n",
       " 'of',\n",
       " 'me',\n",
       " 'i',\n",
       " 'got',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'of',\n",
       " 'him',\n",
       " 'hohum',\n",
       " ',',\n",
       " 'the',\n",
       " 'tune',\n",
       " 'is',\n",
       " 'dumb',\n",
       " 'the',\n",
       " 'words',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mean',\n",
       " 'a',\n",
       " 'thing',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'this',\n",
       " 'a',\n",
       " 'silly',\n",
       " 'song',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'to',\n",
       " 'sing',\n",
       " 'ahhh',\n",
       " 'dip',\n",
       " 'the',\n",
       " 'apple',\n",
       " 'in',\n",
       " 'the',\n",
       " 'brew',\n",
       " '.',\n",
       " 'let',\n",
       " 'the',\n",
       " 'sleeping',\n",
       " 'death',\n",
       " 'seep',\n",
       " 'through',\n",
       " '.',\n",
       " 'look',\n",
       " '!',\n",
       " 'on',\n",
       " 'the',\n",
       " 'skin',\n",
       " '!',\n",
       " 'the',\n",
       " 'symbol',\n",
       " 'of',\n",
       " 'what',\n",
       " 'lies',\n",
       " 'within',\n",
       " '.',\n",
       " 'now',\n",
       " ',',\n",
       " 'turn',\n",
       " 'red',\n",
       " 'to',\n",
       " 'tempt',\n",
       " 'snow',\n",
       " 'white',\n",
       " '.',\n",
       " 'to',\n",
       " 'make',\n",
       " 'her',\n",
       " 'hunger',\n",
       " 'for',\n",
       " 'a',\n",
       " 'bite',\n",
       " '.',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bite',\n",
       " '?',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'for',\n",
       " 'you',\n",
       " '!',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'for',\n",
       " 'snow',\n",
       " 'white',\n",
       " '.',\n",
       " 'when',\n",
       " 'she',\n",
       " 'breaks',\n",
       " 'the',\n",
       " 'tender',\n",
       " 'peel',\n",
       " 'to',\n",
       " 'taste',\n",
       " 'the',\n",
       " 'apple',\n",
       " 'in',\n",
       " 'my',\n",
       " 'hand',\n",
       " ',',\n",
       " 'her',\n",
       " 'breath',\n",
       " 'will',\n",
       " 'still',\n",
       " ',',\n",
       " 'her',\n",
       " 'blood',\n",
       " 'congeal',\n",
       " '.',\n",
       " 'then',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'fairest',\n",
       " 'in',\n",
       " 'the',\n",
       " 'land',\n",
       " '.',\n",
       " 'but',\n",
       " 'wait',\n",
       " '!',\n",
       " 'there',\n",
       " 'may',\n",
       " 'be',\n",
       " 'an',\n",
       " 'antidote',\n",
       " '.',\n",
       " 'nothing',\n",
       " 'must',\n",
       " 'be',\n",
       " 'overlooked',\n",
       " '.',\n",
       " 'ah',\n",
       " '!',\n",
       " 'here',\n",
       " 'it',\n",
       " 'is',\n",
       " '!',\n",
       " 'the',\n",
       " 'victim',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sleeping',\n",
       " 'death',\n",
       " '...',\n",
       " 'can',\n",
       " 'be',\n",
       " 'revived',\n",
       " 'only',\n",
       " 'by',\n",
       " 'love',\n",
       " \"'s\",\n",
       " 'first',\n",
       " 'kiss',\n",
       " '.',\n",
       " 'love',\n",
       " \"'s\",\n",
       " 'first',\n",
       " 'kiss',\n",
       " '!',\n",
       " 'bah',\n",
       " '!',\n",
       " 'no',\n",
       " 'fear',\n",
       " 'of',\n",
       " 'that',\n",
       " '.',\n",
       " 'the',\n",
       " 'dwarfs',\n",
       " 'will',\n",
       " 'think',\n",
       " 'she',\n",
       " \"'s\",\n",
       " 'dead',\n",
       " '.',\n",
       " 'she',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'buried',\n",
       " 'alive',\n",
       " '!',\n",
       " 'buried',\n",
       " 'alive',\n",
       " '!',\n",
       " 'thirsty',\n",
       " '?',\n",
       " 'have',\n",
       " 'a',\n",
       " 'drink',\n",
       " '!',\n",
       " 'the',\n",
       " 'little',\n",
       " 'men',\n",
       " 'will',\n",
       " 'be',\n",
       " 'away',\n",
       " '...',\n",
       " 'and',\n",
       " 'she',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'alone',\n",
       " '...',\n",
       " 'with',\n",
       " 'a',\n",
       " 'harmless',\n",
       " 'old',\n",
       " 'peddler',\n",
       " 'woman',\n",
       " '.',\n",
       " 'a',\n",
       " 'harmless',\n",
       " 'old',\n",
       " 'peddler',\n",
       " 'woman',\n",
       " '.',\n",
       " 'all',\n",
       " 'alone',\n",
       " ',',\n",
       " 'my',\n",
       " 'pet',\n",
       " '?',\n",
       " 'the',\n",
       " '...',\n",
       " 'little',\n",
       " 'men',\n",
       " 'are',\n",
       " 'not',\n",
       " 'here',\n",
       " '?',\n",
       " 'makin',\n",
       " \"'\",\n",
       " 'pies',\n",
       " '?',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'apple',\n",
       " 'pies',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'menfolks',\n",
       " \"'\",\n",
       " 'mouths',\n",
       " 'water',\n",
       " '.',\n",
       " 'pies',\n",
       " 'made',\n",
       " 'from',\n",
       " 'apples',\n",
       " 'like',\n",
       " 'these',\n",
       " '.',\n",
       " 'yes',\n",
       " '!',\n",
       " 'but',\n",
       " 'wait',\n",
       " \"'til\",\n",
       " 'you',\n",
       " 'taste',\n",
       " 'one',\n",
       " ',',\n",
       " 'dearie',\n",
       " '.',\n",
       " 'like',\n",
       " 'to',\n",
       " 'try',\n",
       " 'one',\n",
       " '?',\n",
       " 'go',\n",
       " 'on',\n",
       " '.',\n",
       " 'go',\n",
       " 'on',\n",
       " ',',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bite',\n",
       " '.',\n",
       " 'ah',\n",
       " '!',\n",
       " 'ah',\n",
       " '!',\n",
       " 'oh',\n",
       " '!',\n",
       " 'my',\n",
       " 'heart',\n",
       " '!',\n",
       " 'oh',\n",
       " ',',\n",
       " 'mymy',\n",
       " 'poor',\n",
       " 'heart',\n",
       " '.',\n",
       " 'take',\n",
       " 'me',\n",
       " 'into',\n",
       " 'the',\n",
       " 'house',\n",
       " 'and',\n",
       " 'let',\n",
       " 'me',\n",
       " 'rest',\n",
       " '.',\n",
       " 'a',\n",
       " 'drink',\n",
       " 'of',\n",
       " 'water',\n",
       " ',',\n",
       " 'please',\n",
       " '.',\n",
       " 'and',\n",
       " 'because',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'so',\n",
       " 'good',\n",
       " 'to',\n",
       " 'poor',\n",
       " 'old',\n",
       " 'granny',\n",
       " ',',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'share',\n",
       " 'a',\n",
       " 'secret',\n",
       " 'with',\n",
       " 'you',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'no',\n",
       " 'ordinary',\n",
       " 'apple',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'magic',\n",
       " 'wishing',\n",
       " 'apple',\n",
       " '.',\n",
       " 'yes',\n",
       " '!',\n",
       " 'one',\n",
       " 'bite',\n",
       " 'and',\n",
       " 'all',\n",
       " 'your',\n",
       " 'dreams',\n",
       " 'will',\n",
       " 'come',\n",
       " 'true',\n",
       " '.',\n",
       " 'yes',\n",
       " ',',\n",
       " 'girlie',\n",
       " '!',\n",
       " 'now',\n",
       " ',',\n",
       " 'make',\n",
       " 'a',\n",
       " 'wish',\n",
       " 'and',\n",
       " 'take',\n",
       " 'a',\n",
       " 'bite',\n",
       " '.',\n",
       " 'there',\n",
       " 'must',\n",
       " 'be',\n",
       " 'something',\n",
       " 'your',\n",
       " 'little',\n",
       " 'heart',\n",
       " 'desires',\n",
       " '.',\n",
       " 'perhaps',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'someone',\n",
       " 'you',\n",
       " 'love',\n",
       " '.',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'so',\n",
       " '.',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'so',\n",
       " '.',\n",
       " 'old',\n",
       " 'granny',\n",
       " 'knows',\n",
       " 'a',\n",
       " 'girl',\n",
       " \"'s\",\n",
       " 'heart',\n",
       " '.',\n",
       " 'now',\n",
       " ',',\n",
       " 'take',\n",
       " 'the',\n",
       " 'apple',\n",
       " ',',\n",
       " 'dearie',\n",
       " ',',\n",
       " 'and',\n",
       " 'make',\n",
       " 'a',\n",
       " 'wish',\n",
       " '.',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'it',\n",
       " '.',\n",
       " 'go',\n",
       " 'on',\n",
       " '.',\n",
       " 'go',\n",
       " 'on',\n",
       " '.',\n",
       " 'fine',\n",
       " '!',\n",
       " 'fine',\n",
       " '!',\n",
       " 'now',\n",
       " ',',\n",
       " 'take',\n",
       " 'a',\n",
       " 'bite',\n",
       " '.',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'let',\n",
       " 'the',\n",
       " 'wish',\n",
       " 'grow',\n",
       " 'cold',\n",
       " '!',\n",
       " 'her',\n",
       " 'breath',\n",
       " 'will',\n",
       " 'still',\n",
       " '.',\n",
       " 'her',\n",
       " 'blood',\n",
       " 'congeal',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'fairest',\n",
       " 'in',\n",
       " 'the',\n",
       " 'land',\n",
       " '!',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'trapped',\n",
       " '.',\n",
       " 'what',\n",
       " 'will',\n",
       " 'i',\n",
       " 'do',\n",
       " '?',\n",
       " 'the',\n",
       " 'meddling',\n",
       " 'little',\n",
       " 'fools',\n",
       " '!',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'fix',\n",
       " 'ya',\n",
       " '!',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'crush',\n",
       " 'your',\n",
       " 'bones',\n",
       " '!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_char_df.Total_Toks.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_char_df['Text'] = movie_char_df.Total_Toks.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"slave in the magic mirror come from the farthest space through wind and darkness i summon thee . speak ! let me see thy face . magic mirror on the wall , who is the fairest one of all ? alas for her ! reveal her name . snow white ! take her far into the forest . find some secluded glade where she can pick wildflowers . and there , my faithful huntsman , you will kill her ! silence ! you know the penalty if you fail . but to make doubly sure ... you do not fail , bring back her heart ... magic mirror on the wall , who now is the fairest one of all ? over the seven jewelled hills , beyond the seventh fall , in the cottage of the seven dwarfs , dwells snow white , fairest one of all . snow white lies dead in the forest . the huntsman has brought me proof . behold , her heart . the heart of a pig ! then i 've been tricked ! the heart of a pig ! the blundering fool ! i 'll go myself to the dwarfs ' cottage ... in a disguise so complete no one will ever suspect . now , a formula to transform my beauty into ugliness . change my queenly raiment to a peddler 's cloak . mummy dustto make me old . to shroud my clothes , the black of night . to age my voice , an old hag 's cackle . to whiten my hair , a scream of fright . a blast of wind ... to fan my hate ! a thunderbolt ... to mix it well . now ... begin thy magic spell . look ! my hands ! my voice ! my voice ! a perfect disguise . and now , a special ... sort of death for one so fair . what shall it be ? ah ! a poisoned apple ! sleeping death . one taste of the poisoned apple ... and the victim 's eyes will close forever ... in the sleeping death . yodel holalaeeay holalaeeay holalaeeayeela eeayeeleeay holalaeeay holalaeeay holalaeeayeela leeayleeoleeay i 'd like to dance and tap my feet , but they wo n't keep in rhythm you see , i washed 'em both today and i ca n't do nothin ' with 'em hohum , the tune is dumb the words do n't mean a thing is n't this a silly song for anyone to sing i oh , ggosh ! i chased a polecat up a tree way out upon a limb and when he got the best of me i got the worst of him hohum , the tune is dumb the words do n't mean a thing is n't this a silly song for anyone to sing ahhh dip the apple in the brew . let the sleeping death seep through . look ! on the skin ! the symbol of what lies within . now , turn red to tempt snow white . to make her hunger for a bite . have a bite ? it 's not for you ! it 's for snow white . when she breaks the tender peel to taste the apple in my hand , her breath will still , her blood congeal . then i 'll be fairest in the land . but wait ! there may be an antidote . nothing must be overlooked . ah ! here it is ! the victim of the sleeping death ... can be revived only by love 's first kiss . love 's first kiss ! bah ! no fear of that . the dwarfs will think she 's dead . she 'll be buried alive ! buried alive ! thirsty ? have a drink ! the little men will be away ... and she 'll be alone ... with a harmless old peddler woman . a harmless old peddler woman . all alone , my pet ? the ... little men are not here ? makin ' pies ? it 's apple pies that makes the menfolks ' mouths water . pies made from apples like these . yes ! but wait 'til you taste one , dearie . like to try one ? go on . go on , have a bite . ah ! ah ! oh ! my heart ! oh , mymy poor heart . take me into the house and let me rest . a drink of water , please . and because you 've been so good to poor old granny , i 'll share a secret with you . this is no ordinary apple . it 's a magic wishing apple . yes ! one bite and all your dreams will come true . yes , girlie ! now , make a wish and take a bite . there must be something your little heart desires . perhaps there 's someone you love . i thought so . i thought so . old granny knows a girl 's heart . now , take the apple , dearie , and make a wish . that 's it . go on . go on . fine ! fine ! now , take a bite . do n't let the wish grow cold ! her breath will still . her blood congeal . now i 'll be fairest in the land ! i 'm trapped . what will i do ? the meddling little fools ! i 'll fix ya ! i 'll crush your bones !\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_char_df['Text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = movie_char_df['Text'].iloc[0]\n",
    "lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slave', 'magic', 'mirror', 'come', 'farthest', 'space', 'wind', 'darkness', 'summon', 'let', 'see', 'thy', 'face', 'magic', 'mirror', 'wall', 'be', 'fairest', 'reveal', 'name', 'snow', 'white', 'take', 'far', 'forest', 'find', 'seclude', 'glade', 'pick', 'wildflower', 'faithful', 'huntsman', 'kill', 'silence', 'know', 'penalty', 'fail', 'make', 'doubly', 'sure', 'do', 'not', 'fail', 'bring', 'heart', 'magic', 'mirror', 'wall', 'now', 'be', 'fairest', 'jewelled', 'hill', 'seventh', 'fall', 'cottage', 'dwarf', 'dwell', 'snow', 'white', 'fairest', 'snow', 'white', 'lie', 'dead', 'forest', 'huntsman', 'have', 'bring', 'proof', 'behold', 'heart', 'heart', 'pig', 'then', 've', 'be', 'trick', 'heart', 'pig', 'blundering', 'fool', 'll', 'go', 'dwarf', 'cottage', 'disguise', 'so', 'complete', 'no', 'one', 'ever', 'suspect', 'now', 'formula', 'transform', 'beauty', 'ugliness', 'change', 'queenly', 'raiment', 'peddler', 'cloak', 'mummy', 'make', 'old', 'shroud', 'clothe', 'black', 'night', 'age', 'voice', 'old', 'hag', 'cackle', 'whiten', 'hair', 'scream', 'fright', 'blast', 'wind', 'fan', 'hate', 'thunderbolt', 'mix', 'well', 'now', 'begin', 'thy', 'spell', 'look', 'hand', 'voice', 'voice', 'perfect', 'disguise', 'now', 'special', 'sort', 'death', 'so', 'fair', 'be', 'poison', 'apple', 'sleep', 'death', 'taste', 'poison', 'apple', 'victim', 'eye', 'close', 'forever', 'sleeping', 'death', 'yodel', 'holalaeeay', 'holalaeeay', 'holalaeeayeela', 'eeayeeleeay', 'holalaeeay', 'holalaeeay', 'holalaeeayeela', 'leeayleeoleeay', 'dance', 'tap', 'foot', 'keep', 'rhythm', 'see', 'wash', 'today', 'do', 'nothin', 'hohum', 'tune', 'be', 'dumb', 'word', 'do', 'mean', 'thing', 'be', 'silly', 'song', 'anyone', 'sing', 'ggosh', 'chase', 'polecat', 'tree', 'way', 'limb', 'get', 'best', 'get', 'worst', 'hohum', 'tune', 'be', 'dumb', 'word', 'do', 'mean', 'thing', 'be', 'silly', 'song', 'anyone', 'sing', 'ahhh', 'dip', 'apple', 'brew', 'let', 'sleeping', 'death', 'seep', 'look', 'skin', 'symbol', 'lie', 'now', 'turn', 'red', 'tempt', 'snow', 'white', 'make', 'hunger', 'bite', 'have', 'bite', 'not', 'snow', 'white', 'break', 'tender', 'peel', 'taste', 'apple', 'hand', 'breath', 'still', 'blood', 'congeal', 'then', 'be', 'fairest', 'land', 'be', 'antidote', 'nothing', 'be', 'overlooked', 'here', 'be', 'victim', 'sleeping', 'death', 'be', 'revive', 'only', 'love', 'first', 'kiss', 'love', 'first', 'kiss', 'bah', 'fear', 'dwarf', 'think', 'dead', 'll', 'be', 'bury', 'alive', 'buried', 'alive', 'thirsty', 'have', 'drink', 'little', 'man', 'be', 'away', 'll', 'be', 'alone', 'harmless', 'old', 'peddler', 'woman', 'harmless', 'old', 'peddler', 'woman', 'alone', 'pet', 'little', 'man', 'be', 'not', 'here', 'makin', 'py', 'apple', 'py', 'make', 'mouth', 'water', 'py', 'make', 'apple', 'ye', 'wait', 'taste', 'dearie', 'try', 'go', 'go', 'have', 'bite', 'heart', 'mymy', 'poor', 'heart', 'take', 'house', 'let', 'rest', 'drink', 'water', 'please', 'have', 'be', 'so', 'good', 'poor', 'old', 'granny', 'll', 'share', 'secret', 'be', 'no', 'ordinary', 'apple', 'magic', 'wish', 'apple', 'yes', 'bite', 'dream', 'come', 'true', 'yes', 'girlie', 'now', 'make', 'wish', 'take', 'bite', 'be', 'something', 'little', 'heart', 'desire', 'perhaps', 'someone', 'love', 'think', 'think', 'so', 'old', 'granny', 'know', 'girl', 'heart', 'now', 'take', 'apple', 'dearie', 'make', 'wish', 'go', 'go', 'fine', 'fine', 'now', 'take', 'bite', 'do', 'let', 'wish', 'grow', 'cold', 'breath', 'still', 'blood', 'congeal', 'now', 'be', 'fairest', 'land', 'trap', 'do', 'meddling', 'little', 'fool', 'll', 'fix', 'ya', 'll', 'crush', 'bone']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize all\n",
    "movie_char_df['Lemmas'] = movie_char_df.Text.map(lambda x: [wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_chars = movie_char_df[movie_char_df.Gender == 'f']\n",
    "m_chars = movie_char_df[movie_char_df.Gender == 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features = 500, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_char_df['Lemmas'] = movie_char_df.Lemmas.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slave magic mirror come farthest space wind da...\n",
       "1    wouldst know queen fame be thy beauty majesty ...\n",
       "2                                           today song\n",
       "3    matter mama papa believe re lose please do com...\n",
       "4    yes majesty majesty little princess majesty do...\n",
       "Name: Lemmas, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_char_df.Lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tfidf = tfidf_vectorizer.fit_transform(movie_char_df['Lemmas'])\n",
    "m_tfidf = tfidf_vectorizer.fit_transform(movie_char_df['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, tfidf_vectorizer, n_top_words):\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Picking number of topics and number of words\n",
    "number_topics = 5\n",
    "number_words = 5\n",
    "\n",
    "#lda = LDA(n_components=number_topics)\n",
    "#lda.fit(movies_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(f_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "princess look king queen dream\n",
      "\n",
      "Topic #1:\n",
      "ll just know come right\n",
      "\n",
      "Topic #2:\n",
      "warrior po master dragon fish\n",
      "\n",
      "Topic #3:\n",
      "hiccup little trouble boy far\n",
      "\n",
      "Topic #4:\n",
      "moana ow girl pocahontas jack\n"
     ]
    }
   ],
   "source": [
    "print(\"Female Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(m_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "day queen bala jack snow\n",
      "\n",
      "Topic #1:\n",
      "little general dig today alive\n",
      "\n",
      "Topic #2:\n",
      "come just ll know right\n",
      "\n",
      "Topic #3:\n",
      "hiccup princess sir dragon mulan\n",
      "\n",
      "Topic #4:\n",
      "worker warrior look girl dragon\n"
     ]
    }
   ],
   "source": [
    "print(\"Male Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Across roles?\n",
    "pro_chars = movie_char_df[movie_char_df.Role == 'PRO']\n",
    "ant_chars = movie_char_df[movie_char_df.Role == 'ANT']\n",
    "help_chars = movie_char_df[movie_char_df.Role == 'HELPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_tfidf = tfidf_vectorizer.fit_transform(pro_chars['Lemmas'])\n",
    "ant_tfidf = tfidf_vectorizer.fit_transform(ant_chars['Lemmas'])\n",
    "help_tfidf = tfidf_vectorizer.fit_transform(help_chars['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(pro_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pro Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "snow mighty idea tigress hope\n",
      "\n",
      "Topic #1:\n",
      "fiona dream crazy dragon gusgus\n",
      "\n",
      "Topic #2:\n",
      "ant thank tonight egg protect\n",
      "\n",
      "Topic #3:\n",
      "hope trap voodoo bad born\n",
      "\n",
      "Topic #4:\n",
      "door point smell matter thought\n"
     ]
    }
   ],
   "source": [
    "print(\"Pro Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANT Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "knock donkey kristoff ooh arm\n",
      "\n",
      "Topic #1:\n",
      "way bye ve kristoff fight\n",
      "\n",
      "Topic #2:\n",
      "lay win mouth giant lumiere\n",
      "\n",
      "Topic #3:\n",
      "hurry kristoff daddy hope arm\n",
      "\n",
      "Topic #4:\n",
      "watch safe mmm dark island\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(ant_tfidf)\n",
    "\n",
    "print(\"ANT Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELPER Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "mate jack daddy chirp yah\n",
      "\n",
      "Topic #1:\n",
      "know ll just come dragon\n",
      "\n",
      "Topic #2:\n",
      "dad shh believe lung panda\n",
      "\n",
      "Topic #3:\n",
      "da slipper pocahontas eric aladdin\n",
      "\n",
      "Topic #4:\n",
      "use oooh rise team shut\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(help_tfidf)\n",
    "\n",
    "print(\"HELPER Topics found via LDA:\")\n",
    "print_topics(lda, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
